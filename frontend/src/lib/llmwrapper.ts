/**
 * LLM Wrapper for unified frontend LLM API calls
 * 
 * Provides both streaming and non-streaming interfaces for LLM interactions.
 * - Streaming: Used for conversational chat interfaces
 * - Non-streaming: Used for agent-type operations like file summarization
 */

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface ChatCompletionOptions {
  messages: ChatMessage[]
  stream?: boolean
  maxTokens?: number
  temperature?: number
  model?: string
  apiKey?: string
  baseUrl?: string
}

export interface ChatCompletionResponse {
  id: string
  object: string
  created: number
  model: string
  choices: Array<{
    index: number
    message?: ChatMessage
    finish_reason?: string
  }>
  usage?: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
  }
}

export interface StreamChunk {
  id: string
  object: string
  created: number
  model: string
  choices: Array<{
    index: number
    delta: {
      content?: string
    }
    finish_reason?: string
  }>
}

export interface ModelInfo {
  id: string
  object: string
  created: number
  owned_by: string
  permission: unknown[]
  root: string
  parent: string | null
}

export interface ModelsResponse {
  object: string
  data: ModelInfo[]
}

export interface TranslationRequest {
  text: string
  source_language: string
  target_language: string
  model?: string
}

export interface TranslationResult {
  original_text: string
  translated_text: string
  source_language: string
  target_language: string
  model: string
  confidence?: number
  is_complete?: boolean
}

export interface SupportedLanguage {
  code: string
  name: string
  native_name: string
  flag?: string
}

export class LLMWrapper {
  private baseUrl: string

  constructor() {
    // Use the same base URL as the main API
    this.baseUrl = 'http://localhost:8001'
  }

  /**
   * Non-streaming chat completion (for agent-type operations)
   */
  async chat(options: ChatCompletionOptions): Promise<string> {
    try {
      const response = await this.makeRequest({
        ...options,
        stream: false
      })

      const data = await response.json() as ChatCompletionResponse
      return data.choices[0]?.message?.content || ''
    } catch (error) {
      console.error('LLM chat error:', error)
      throw new Error(`LLM chat failed: ${error}`)
    }
  }

  /**
   * Streaming chat completion (for conversational interfaces)
   */
  async streamChat(options: ChatCompletionOptions): Promise<ReadableStream<string>> {
    try {
      const response = await this.makeRequest({
        ...options,
        stream: true
      })

      return this.createStreamProcessor(response)
    } catch (error) {
      console.error('LLM stream chat error:', error)
      throw new Error(`LLM stream chat failed: ${error}`)
    }
  }

  /**
   * Summarize file content (streaming)
   */
  async streamSummarizeFile(content: string, maxLength = 10000, model?: string, apiKey?: string, baseUrl?: string): Promise<ReadableStream<string>> {
    // Truncate content if too long
    const truncatedContent = this.truncateContent(content, maxLength)
    
    const messages: ChatMessage[] = [
      {
        role: 'system',
        content: `ç”¨æˆ·å°†æä¾›ä¸€ç¯‡æ–‡æ¡£ï¼Œè¯·å‘Šè¯‰è¯»è€…è¿™ç¯‡æ–‡æ¡£çš„æœ‰ä»€ä¹ˆå†…å®¹ï¼Œç»“åˆæœ‰åºåˆ—è¡¨ã€åŠ ç²—å­—ä½“ç®€è¦é™ˆåˆ—æ–‡æ¡£çš„ä¸»è¦ä¿¡æ¯æˆ–è§‚ç‚¹ï¼Œè¯­è¨€ç®€æ´æ˜äº†ã€‚`
      },
      {
        role: 'user',
        content: `${truncatedContent}\n\n è¯·æ¦‚æ‹¬ä¸Šè¿°å†…å®¹ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šï¼Œç›´æ¥è¾“å‡ºæ–‡æ¡£æ¦‚è¦ï¼š`
      }
    ]

    console.log('Creating stream for file summarization...')
    const stream = await this.streamChat({
      messages,
      temperature: 0.3,
      maxTokens: 2000,
      model,
      apiKey,
      baseUrl
    })
    console.log('Stream created successfully')
    return stream
  }

  /**
   * Extract keywords from user query (non-streaming)
   */
  async extractKeywords(query: string, model?: string, apiKey?: string, baseUrl?: string): Promise<string[][]> {
    const messages: ChatMessage[] = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…³é”®è¯æå–åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œæå–ä¸åŒé•¿åº¦çš„å…³é”®è¯ç»„åˆç”¨äºæ–‡ä»¶æœç´¢ã€‚

**é‡è¦è§„åˆ™ï¼š**
1. **åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—**
2. ç”Ÿæˆ2-4ç»„ä¸åŒé•¿åº¦çš„å…³é”®è¯ç»„åˆ
3. åŒ…å«ï¼š
   - 1-2ç»„ä¸¤ä¸ªå…³é”®è¯ï¼ˆæœ€æ ¸å¿ƒçš„2ä¸ªè¯æ±‡ï¼Œæé«˜å¬å›ç‡ï¼‰
   - 1-2ç»„ä¸‰ä¸ªå…³é”®è¯ï¼ˆæé«˜ç²¾ç¡®åº¦ï¼Œç”¨äºç²¾å‡†æœç´¢ï¼‰
4. åŒ…å«åŒä¹‰è¯ã€è‹±æ–‡ç¼©å†™ã€ç›¸å…³æ¦‚å¿µ
5. è€ƒè™‘ä¸åŒçš„æœç´¢è§’åº¦å’Œè¡¨è¾¾æ–¹å¼

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§æ­¤æ ¼å¼ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼‰ï¼š**
[
  ["æ ¸å¿ƒè¯", "ç›¸å…³è¯"],
  ["æŠ€æœ¯è¯", "åº”ç”¨è¯"],
  ["æ¦‚å¿µè¯", "æ–¹æ³•è¯", "é¢†åŸŸè¯"]
]

**ç¤ºä¾‹ï¼š**
æŸ¥è¯¢ï¼š"æœºå™¨å­¦ä¹ ç®—æ³•"
è¾“å‡ºï¼š
[
  ["æœºå™¨å­¦ä¹ ", "ç®—æ³•"],
  ["æ·±åº¦å­¦ä¹ ", "è®­ç»ƒ"],
  ["ç¥ç»ç½‘ç»œ", "é¢„æµ‹", "åˆ†ç±»"]
]`
      },
      {
        role: 'user',
        content: `ç”¨æˆ·æŸ¥è¯¢ï¼š"${query}"\n\nè¯·æå–å…³é”®è¯ç»„åˆï¼ˆåªè¿”å›JSONæ•°ç»„ï¼‰ï¼š`
      }
    ]

    try {
      const response = await this.chat({
        messages,
        temperature: 0.3,
        maxTokens: 400,
        model,
        apiKey,
        baseUrl
      })

      // Clean and parse JSON response
      let cleanedResponse = response.trim()
      
      // Try to extract JSON array from response if it contains extra text
      const jsonMatch = cleanedResponse.match(/\[[\s\S]*\]/)
      if (jsonMatch) {
        cleanedResponse = jsonMatch[0]
      }
      
      console.log('Keywords extraction response:', cleanedResponse)
      const parsed = JSON.parse(cleanedResponse)
      
      if (Array.isArray(parsed) && parsed.every(group => Array.isArray(group))) {
        // Filter out empty groups and ensure each group has valid keywords
        const validGroups = parsed
          .filter(group => group.length > 0)
          .map(group => group.filter(keyword => typeof keyword === 'string' && keyword.trim().length > 0))
          .filter(group => group.length > 0)
        
        if (validGroups.length > 0) {
          return validGroups as string[][]
        }
      }
      
      throw new Error('Invalid response format')
    } catch (error) {
      console.warn('Failed to extract keywords, using fallback:', error)
      // Enhanced fallback: create multiple search strategies
      const baseKeywords = query.split(/\s+/).filter(word => word.length > 1)
      return [
        baseKeywords, // Original query words
        [query], // Full query as single term
        baseKeywords.slice(0, 2) // First two words only
      ].filter(group => group.length > 0)
    }
  }

  /**
   * Analyze file relevance and provide recommendations (non-streaming)
   */
  async analyzeRelevance(userQuery: string, files: Array<{ file_path: string; file_name: string; file_type?: string; content_preview?: string; match_score?: number }>, model?: string, apiKey?: string, baseUrl?: string): Promise<{
    reasoning: string
    recommendedFiles: Array<{ file_path: string; file_name: string; file_type?: string; content_preview?: string; match_score?: number }>
  }> {
    const filesInfo = files.slice(0, 20).map(file => ({
      path: file.file_path,
      name: file.file_name,
      type: file.file_type,
      preview: file.content_preview || '',
      score: file.match_score
    }))

    const messages: ChatMessage[] = [
      {
        role: 'system',
        content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶ç›¸å…³æ€§åˆ†æåŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·æŸ¥è¯¢å’Œæœç´¢ç»“æœï¼Œåˆ†ææ–‡ä»¶ç›¸å…³æ€§å¹¶æä¾›æ¨èã€‚

**é‡è¦è§„åˆ™ï¼š**
1. **åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—**
2. åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾å’Œéœ€æ±‚
3. è¯„ä¼°æ¯ä¸ªæ–‡ä»¶çš„ç›¸å…³æ€§ç¨‹åº¦
4. æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºï¼Œæ¨è5-10ä¸ªæœ€ç›¸å…³æ–‡ä»¶
5. æä¾›ç®€æ´æ¸…æ™°çš„æ¨èç†ç”±

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§æ­¤æ ¼å¼ï¼‰ï¼š**
{
  "reasoning": "åŸºäºç”¨æˆ·æŸ¥è¯¢åˆ†æï¼Œæ¨èä»¥ä¸‹æ–‡ä»¶çš„ç†ç”±",
  "recommendedFiles": ["æ–‡ä»¶è·¯å¾„1", "æ–‡ä»¶è·¯å¾„2", "æ–‡ä»¶è·¯å¾„3"]
}

**åˆ†æè¦ç‚¹ï¼š**
- æ–‡ä»¶åä¸æŸ¥è¯¢çš„åŒ¹é…åº¦
- æ–‡ä»¶ç±»å‹çš„ç›¸å…³æ€§
- å†…å®¹é¢„è§ˆçš„ç›¸å…³æ€§
- æ–‡ä»¶çš„åŒ¹é…è¯„åˆ†`
      },
      {
        role: 'user',
        content: `ç”¨æˆ·æŸ¥è¯¢ï¼š"${userQuery}"

å¯é€‰æ–‡ä»¶åˆ—è¡¨ï¼š
${JSON.stringify(filesInfo, null, 2)}

è¯·åˆ†æå¹¶æ¨èæœ€ç›¸å…³çš„æ–‡ä»¶ï¼ˆåªè¿”å›JSONæ ¼å¼ï¼‰ï¼š`
      }
    ]

    try {
      const response = await this.chat({
        messages,
        temperature: 0.3,
        maxTokens: 600,
        model,
        apiKey,
        baseUrl
      })

      // Clean and parse JSON response
      let cleanedResponse = response.trim()
      
      // Try to extract JSON object from response if it contains extra text
      const jsonMatch = cleanedResponse.match(/\{[\s\S]*\}/)
      if (jsonMatch) {
        cleanedResponse = jsonMatch[0]
      }
      
      console.log('File relevance analysis response:', cleanedResponse)
      const parsed = JSON.parse(cleanedResponse)
      
      if (parsed.reasoning && Array.isArray(parsed.recommendedFiles)) {
        // Map file paths back to full file objects
        const recommendedFiles = parsed.recommendedFiles
          .map((filePath: string) => files.find(f => f.file_path === filePath))
          .filter(Boolean)
          .slice(0, 10) // Limit to 10 files max

        return {
          reasoning: parsed.reasoning,
          recommendedFiles
        }
      } else {
        throw new Error('Invalid response format')
      }
    } catch (error) {
      console.warn('Failed to analyze relevance, using fallback:', error)
      // Enhanced fallback: return top files by match score with better reasoning
      const topFiles = files
        .slice(0, 10)
        .sort((a, b) => (b.match_score || 0) - (a.match_score || 0))
      
      return {
        reasoning: `åŸºäºæœç´¢åŒ¹é…åº¦å’Œæ–‡ä»¶ç±»å‹ç›¸å…³æ€§ï¼Œä¸ºæŸ¥è¯¢"${userQuery}"æ¨èä»¥ä¸‹${topFiles.length}ä¸ªæ–‡ä»¶ã€‚æ¨èé¡ºåºæŒ‰åŒ¹é…åº¦ä»é«˜åˆ°ä½æ’åˆ—ã€‚`,
        recommendedFiles: topFiles
      }
    }
  }

  /**
   * Truncate content intelligently
   */
  private truncateContent(content: string, maxLength: number): string {
    if (content.length <= maxLength) {
      return content
    }

    // Intelligent truncation: prioritize beginning and end
    const startLength = Math.floor(maxLength * 0.7)
    const endLength = maxLength - startLength - 50 // 50 chars for ellipsis

    return content.substring(0, startLength) + 
           '\n\n... [ä¸­é—´å†…å®¹å·²çœç•¥] ...\n\n' + 
           content.substring(content.length - endLength)
  }

  /**
   * Create stream processor for handling SSE responses
   */
  private createStreamProcessor(response: Response): ReadableStream<string> {
    if (!response.body) {
      throw new Error('No response body received')
    }

    const reader = response.body.getReader()
    const decoder = new TextDecoder()

    return new ReadableStream<string>({
      start(controller) {
        let buffer = ''
        
        const processStream = async () => {
          try {
            let isReading = true;
            while (isReading) {
              const { done, value } = await reader.read()
              
              if (done) {
                // Process any remaining data in buffer
                if (buffer.trim()) {
                  console.warn('Unprocessed data in buffer:', buffer)
                }
                controller.close()
                isReading = false;
                break
              }

              const chunk = decoder.decode(value, { stream: true })
              buffer += chunk
              
              // Process complete lines
              const lines = buffer.split('\n')
              buffer = lines.pop() || '' // Keep incomplete line in buffer

              for (const line of lines) {
                const trimmedLine = line.trim()
                if (!trimmedLine) continue
                
                if (trimmedLine.startsWith('data: ')) {
                  const data = trimmedLine.slice(6)
                  
                  if (data === '[DONE]') {
                    controller.close()
                    return
                  }

                  try {
                    const parsed = JSON.parse(data) as StreamChunk
                    const content = parsed.choices[0]?.delta?.content
                    
                    if (content) {
                      console.log('[LLMWrapper] Enqueuing content chunk:', JSON.stringify(content))
                      controller.enqueue(content)
                    }
                  } catch (parseError) {
                    console.warn('Failed to parse stream chunk:', data, parseError)
                  }
                }
              }
            }
          } catch (error) {
            console.error('[LLMWrapper] Stream processing error:', error)
            controller.error(error)
          } finally {
            // Clean up the original reader
            try {
              reader.releaseLock()
            } catch (e) {
              console.warn('Error releasing original reader:', e)
            }
          }
        }
        
        processStream()
      }
    })
  }

  /**
   * Translate text using LLM (non-streaming)
   */
  async translateText(request: TranslationRequest): Promise<TranslationResult> {
    try {
      const response = await fetch(`${this.baseUrl}/api/llm/translate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          text: request.text,
          source_language: request.source_language,
          target_language: request.target_language,
          model: request.model
        })
      })

      if (!response.ok) {
        const errorData = await response.json()
        throw new Error(errorData.detail || `HTTP ${response.status}`)
      }

      const result = await response.json() as TranslationResult
      return result
    } catch (error) {
      console.error('Translation failed:', error)
      throw new Error(`Translation failed: ${error}`)
    }
  }

  /**
   * Stream translate text using LLM
   */
  async streamTranslateText(request: TranslationRequest): Promise<ReadableStream<TranslationResult>> {
    try {
      const response = await fetch(`${this.baseUrl}/api/llm/translate?stream=true`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          text: request.text,
          source_language: request.source_language,
          target_language: request.target_language,
          model: request.model
        })
      })

      if (!response.ok) {
        const errorData = await response.json()
        throw new Error(errorData.detail || `HTTP ${response.status}`)
      }

      return this.createTranslationStreamProcessor(response)
    } catch (error) {
      console.error('Streaming translation failed:', error)
      throw new Error(`Streaming translation failed: ${error}`)
    }
  }

  /**
   * Detect language of given text
   */
  async detectLanguage(text: string): Promise<string> {
    try {
      const response = await fetch(`${this.baseUrl}/api/llm/detect-language`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ text })
      })

      if (!response.ok) {
        const errorData = await response.json()
        throw new Error(errorData.detail || `HTTP ${response.status}`)
      }

      const result = await response.json()
      return result.language
    } catch (error) {
      console.error('Language detection failed:', error)
      return 'unknown'
    }
  }

  /**
   * Get available models from the LLM API
   */
  async getAvailableModels(): Promise<ModelInfo[]> {
    try {
      const response = await fetch(`${this.baseUrl}/api/llm/models`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json'
        }
      })

      if (!response.ok) {
        const errorData = await response.json()
        throw new Error(errorData.detail || `HTTP ${response.status}`)
      }

      const models = await response.json() as ModelInfo[]
      return models
    } catch (error) {
      console.error('Failed to get available models:', error)
      return []
    }
  }

  /**
   * Get supported languages for translation
   */
  getSupportedLanguages(): SupportedLanguage[] {
    return [
      { code: 'auto', name: 'Auto Detect', native_name: 'Auto Detect' },
      { code: 'en', name: 'English', native_name: 'English', flag: 'ğŸ‡ºğŸ‡¸' },
      { code: 'zh', name: 'Chinese', native_name: 'ä¸­æ–‡', flag: 'ğŸ‡¨ğŸ‡³' },
      { code: 'ja', name: 'Japanese', native_name: 'æ—¥æœ¬èª', flag: 'ğŸ‡¯ğŸ‡µ' },
      { code: 'ko', name: 'Korean', native_name: 'í•œêµ­ì–´', flag: 'ğŸ‡°ğŸ‡·' },
      { code: 'es', name: 'Spanish', native_name: 'EspaÃ±ol', flag: 'ğŸ‡ªğŸ‡¸' },
      { code: 'fr', name: 'French', native_name: 'FranÃ§ais', flag: 'ğŸ‡«ğŸ‡·' },
      { code: 'de', name: 'German', native_name: 'Deutsch', flag: 'ğŸ‡©ğŸ‡ª' },
      { code: 'it', name: 'Italian', native_name: 'Italiano', flag: 'ğŸ‡®ğŸ‡¹' },
      { code: 'pt', name: 'Portuguese', native_name: 'PortuguÃªs', flag: 'ğŸ‡µğŸ‡¹' },
      { code: 'ru', name: 'Russian', native_name: 'Ğ ÑƒÑÑĞºĞ¸Ğ¹', flag: 'ğŸ‡·ğŸ‡º' },
      { code: 'ar', name: 'Arabic', native_name: 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', flag: 'ğŸ‡¸ğŸ‡¦' },
      { code: 'hi', name: 'Hindi', native_name: 'à¤¹à¤¿à¤¨à¥à¤¦à¥€', flag: 'ğŸ‡®ğŸ‡³' },
      { code: 'th', name: 'Thai', native_name: 'à¹„à¸—à¸¢', flag: 'ğŸ‡¹ğŸ‡­' },
      { code: 'vi', name: 'Vietnamese', native_name: 'Tiáº¿ng Viá»‡t', flag: 'ğŸ‡»ğŸ‡³' }
    ]
  }

  /**
   * Create stream processor for translation responses
   */
  private createTranslationStreamProcessor(response: Response): ReadableStream<TranslationResult> {
    if (!response.body) {
      throw new Error('No response body received')
    }

    const reader = response.body.getReader()
    const decoder = new TextDecoder()

    return new ReadableStream<TranslationResult>({
      start(controller) {
        let buffer = ''
        
        const processStream = async () => {
          try {
            let isReading = true;
            while (isReading) {
              const { done, value } = await reader.read()
              
              if (done) {
                controller.close()
                isReading = false;
                break
              }

              const chunk = decoder.decode(value, { stream: true })
              buffer += chunk
              
              // Process complete lines
              const lines = buffer.split('\n')
              buffer = lines.pop() || ''

              for (const line of lines) {
                const trimmedLine = line.trim()
                if (!trimmedLine) continue
                
                if (trimmedLine.startsWith('data: ')) {
                  const data = trimmedLine.slice(6)
                  
                  if (data === '[DONE]') {
                    controller.close()
                    return
                  }

                  try {
                    const parsed = JSON.parse(data) as TranslationResult
                    console.log('[LLMWrapper] Translation chunk received:', parsed)
                    controller.enqueue(parsed)
                  } catch (parseError) {
                    console.warn('Failed to parse translation chunk:', data, parseError)
                  }
                }
              }
            }
          } catch (error) {
            console.error('[LLMWrapper] Translation stream processing error:', error)
            controller.error(error)
          } finally {
            try {
              reader.releaseLock()
            } catch (e) {
              console.warn('Error releasing translation reader:', e)
            }
          }
        }
        
        processStream()
      }
    })
  }

  /**
   * Get available models from the API
   */
  async getModels(apiKey?: string, baseUrl?: string): Promise<ModelInfo[]> {
    try {
      // Build request body, only include credentials if provided
      const requestBody: { api_key?: string; base_url?: string } = {}
      
      if (apiKey) {
        requestBody.api_key = apiKey
      }
      
      if (baseUrl) {
        requestBody.base_url = baseUrl
      }
      
      console.log('[LLMWrapper] Getting models with:', {
        hasApiKey: !!apiKey,
        baseUrl: baseUrl || 'backend default'
      })

      const response = await fetch(`${this.baseUrl}/v1/models`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestBody)
      })

      if (!response.ok) {
        const errorData = await response.json()
        throw new Error(errorData.detail || `HTTP ${response.status}`)
      }

      const data = await response.json() as ModelsResponse
      return data.data || []
    } catch (error) {
      console.error('Failed to get models:', error)
      // Return fallback models if API call fails
      return [
        { id: 'gpt-4.1-nano', object: 'model', created: Date.now(), owned_by: 'openai', permission: [] as unknown[], root: 'gpt-4.1-nano', parent: null },
        { id: 'gpt-4.1-mini', object: 'model', created: Date.now(), owned_by: 'openai', permission: [] as unknown[], root: 'gpt-4.1-mini', parent: null }
      ]
    }
  }

  /**
   * Make HTTP request to LLM API (unified for both streaming and non-streaming)
   */
  private async makeRequest(options: ChatCompletionOptions): Promise<Response> {
    // Build request body, only include model if explicitly provided
    const requestBody: {
      messages: ChatMessage[];
      stream: boolean;
      max_tokens?: number;
      temperature: number;
      model?: string;
      api_key?: string;
      base_url?: string;
    } = {
      messages: options.messages,
      stream: options.stream || false,
      max_tokens: options.maxTokens,
      temperature: options.temperature || 0.7
    }
    
    // Only include optional fields if they're explicitly provided
    if (options.model) {
      requestBody.model = options.model
    }
    
    if (options.apiKey) {
      requestBody.api_key = options.apiKey
    }
    
    if (options.baseUrl) {
      requestBody.base_url = options.baseUrl
    }
    
    console.log('[LLMWrapper] Sending request with:', {
      model: options.model || 'backend default',
      hasApiKey: !!options.apiKey,
      baseUrl: options.baseUrl || 'backend default'
    })
    
    const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody)
    })

    if (!response.ok) {
      const errorData = await response.json()
      throw new Error(errorData.detail || `HTTP ${response.status}`)
    }

    return response
  }
}

// Export singleton instance
export const llmWrapper = new LLMWrapper()