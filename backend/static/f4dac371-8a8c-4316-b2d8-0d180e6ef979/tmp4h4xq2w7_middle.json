{
    "pdf_info": [
        {
            "preproc_blocks": [
                {
                    "type": "title",
                    "bbox": [
                        33,
                        17,
                        236,
                        40
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                17,
                                236,
                                40
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        17,
                                        236,
                                        40
                                    ],
                                    "score": 1.0,
                                    "content": "大语言模型科普报告",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        71,
                        536,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                71,
                                536,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        71,
                                        536,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "帮我做一份详细的大语言模型的科普报告，内容结构不做要求， 但是必须包含以下内容：",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        125,
                        420,
                        137
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                125,
                                420,
                                137
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        125,
                                        420,
                                        137
                                    ],
                                    "score": 1.0,
                                    "content": "1、介绍大语言模型的发展史和基本原理",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        143,
                        539,
                        173
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                143,
                                539,
                                173
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        143,
                                        539,
                                        173
                                    ],
                                    "score": 1.0,
                                    "content": "2、对于常用的基础的概念（不低于80个）要做出详细且通俗易 懂的解释。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        180,
                        500,
                        192
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                180,
                                500,
                                192
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        180,
                                        500,
                                        192
                                    ],
                                    "score": 1.0,
                                    "content": "3、对于大语言模型的benchmark评价指标，做出解释。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        198,
                        538,
                        245
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                198,
                                538,
                                245
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        198,
                                        538,
                                        245
                                    ],
                                    "score": 1.0,
                                    "content": "benchmark参考最新的deepseek R1技术报告中涉及的，以及 OpenAI o1模型、o3 mini模型中出现的benchmark，如GPQA Diamond等新指标。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        251,
                        531,
                        281
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                251,
                                531,
                                281
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        251,
                                        531,
                                        281
                                    ],
                                    "score": 1.0,
                                    "content": "4、对模型的技术细节也要介绍，开源的闭源的，不同架构的， 都要详细介绍清楚。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        287,
                        534,
                        336
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                287,
                                534,
                                336
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        287,
                                        534,
                                        336
                                    ],
                                    "score": 1.0,
                                    "content": "5、介绍一些主流的大模型（2024年及以后发布的，GPT4o、 OpenAI o1、OpenAI o3-mini、Gemini 2.0 Pro、Claude 3.5 sonnet、Llama3、DeepSeek R1、Kimi K1.5等）",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        359,
                        429,
                        372
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                359,
                                429,
                                372
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        359,
                                        429,
                                        372
                                    ],
                                    "score": 1.0,
                                    "content": "要求只参考英文信息源，最后用中文输出。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        389,
                        558,
                        422
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                389,
                                558,
                                422
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        389,
                                        558,
                                        422
                                    ],
                                    "score": 1.0,
                                    "content": "这个报告将涵盖大语言模型的发展历史、基础概念、benchmark评价指标、技术细节以及2024年及 以后的主流大模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        437,
                        321,
                        449
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                437,
                                321,
                                449
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        437,
                                        321,
                                        449
                                    ],
                                    "score": 1.0,
                                    "content": "为了确保内容全面且符合您的要求，请确认以下几点：",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10
                },
                {
                    "type": "text",
                    "bbox": [
                        40,
                        463,
                        551,
                        497
                    ],
                    "lines": [
                        {
                            "bbox": [
                                40,
                                463,
                                551,
                                497
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        40,
                                        463,
                                        551,
                                        497
                                    ],
                                    "score": 1.0,
                                    "content": "1. 深度：您希望对技术细节的讨论深入到论文级别（如具体的架构优化、训练方法）还是更偏向 科普性（如关键技术点的原理和应用）？",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        511,
                        502,
                        524
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                511,
                                502,
                                524
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        511,
                                        502,
                                        524
                                    ],
                                    "score": 1.0,
                                    "content": "2. 格式：您是否有偏好的格式？比如是否希望以章节划分，或者希望以问答形式呈现？",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        539,
                        555,
                        572
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                539,
                                555,
                                572
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        539,
                                        555,
                                        572
                                    ],
                                    "score": 1.0,
                                    "content": "3. benchmark 详尽度：是否需要具体的数值比较（如模型在不同任务上的具体分数），还是只需 介绍这些benchmark的作用和测试方法？",
                                    "type": "text"
                                }
                            ],
                            "index": 13
                        }
                    ],
                    "index": 13
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        587,
                        490,
                        599
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                587,
                                490,
                                599
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        587,
                                        490,
                                        599
                                    ],
                                    "score": 1.0,
                                    "content": "4. 语言：您希望整体用中文输出，但是否希望附带关键术语的英文原文，以便理解？",
                                    "type": "text"
                                }
                            ],
                            "index": 14
                        }
                    ],
                    "index": 14
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        614,
                        525,
                        626
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                614,
                                525,
                                626
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        614,
                                        525,
                                        626
                                    ],
                                    "score": 1.0,
                                    "content": "5. 目标读者：这个报告是面向一般公众，还是技术从业者，如机器学习工程师和研究人员？",
                                    "type": "text"
                                }
                            ],
                            "index": 15
                        }
                    ],
                    "index": 15
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        650,
                        292,
                        662
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                650,
                                292,
                                662
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        650,
                                        292,
                                        662
                                    ],
                                    "score": 1.0,
                                    "content": "请确认或补充您的需求，以便更精准地提供报告。",
                                    "type": "text"
                                }
                            ],
                            "index": 16
                        }
                    ],
                    "index": 16
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        688,
                        543,
                        737
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                688,
                                543,
                                737
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        688,
                                        543,
                                        737
                                    ],
                                    "score": 1.0,
                                    "content": "深度：您希望对技术细节的讨论深入到论文级别（如具体的架构 优化、训练方法）还是更偏向科普性（如关键技术点的原理和应 用）？",
                                    "type": "text"
                                }
                            ],
                            "index": 17
                        }
                    ],
                    "index": 17
                },
                {
                    "type": "text",
                    "bbox": [
                        206,
                        743,
                        291,
                        755
                    ],
                    "lines": [
                        {
                            "bbox": [
                                206,
                                743,
                                291,
                                755
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        206,
                                        743,
                                        291,
                                        755
                                    ],
                                    "score": 1.0,
                                    "content": "偏向科普性即可",
                                    "type": "text"
                                }
                            ],
                            "index": 18
                        }
                    ],
                    "index": 18
                }
            ],
            "layout_bboxes": [],
            "page_idx": 0,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "title",
                    "bbox": [
                        33,
                        17,
                        236,
                        40
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                17,
                                236,
                                40
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        17,
                                        236,
                                        40
                                    ],
                                    "score": 1.0,
                                    "content": "大语言模型科普报告",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        71,
                        536,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                71,
                                536,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        71,
                                        536,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "帮我做一份详细的大语言模型的科普报告，内容结构不做要求， 但是必须包含以下内容：",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        125,
                        420,
                        137
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                125,
                                420,
                                137
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        125,
                                        420,
                                        137
                                    ],
                                    "score": 1.0,
                                    "content": "1、介绍大语言模型的发展史和基本原理",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        143,
                        539,
                        173
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                143,
                                539,
                                173
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        143,
                                        539,
                                        173
                                    ],
                                    "score": 1.0,
                                    "content": "2、对于常用的基础的概念（不低于80个）要做出详细且通俗易 懂的解释。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        180,
                        500,
                        192
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                180,
                                500,
                                192
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        180,
                                        500,
                                        192
                                    ],
                                    "score": 1.0,
                                    "content": "3、对于大语言模型的benchmark评价指标，做出解释。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        198,
                        538,
                        245
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                198,
                                538,
                                245
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        198,
                                        538,
                                        245
                                    ],
                                    "score": 1.0,
                                    "content": "benchmark参考最新的deepseek R1技术报告中涉及的，以及 OpenAI o1模型、o3 mini模型中出现的benchmark，如GPQA Diamond等新指标。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        251,
                        531,
                        281
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                251,
                                531,
                                281
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        251,
                                        531,
                                        281
                                    ],
                                    "score": 1.0,
                                    "content": "4、对模型的技术细节也要介绍，开源的闭源的，不同架构的， 都要详细介绍清楚。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        287,
                        534,
                        336
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                287,
                                534,
                                336
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        287,
                                        534,
                                        336
                                    ],
                                    "score": 1.0,
                                    "content": "5、介绍一些主流的大模型（2024年及以后发布的，GPT4o、 OpenAI o1、OpenAI o3-mini、Gemini 2.0 Pro、Claude 3.5 sonnet、Llama3、DeepSeek R1、Kimi K1.5等）",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        359,
                        429,
                        372
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                359,
                                429,
                                372
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        359,
                                        429,
                                        372
                                    ],
                                    "score": 1.0,
                                    "content": "要求只参考英文信息源，最后用中文输出。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        389,
                        558,
                        422
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                389,
                                558,
                                422
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        389,
                                        558,
                                        422
                                    ],
                                    "score": 1.0,
                                    "content": "这个报告将涵盖大语言模型的发展历史、基础概念、benchmark评价指标、技术细节以及2024年及 以后的主流大模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        437,
                        321,
                        449
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                437,
                                321,
                                449
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        437,
                                        321,
                                        449
                                    ],
                                    "score": 1.0,
                                    "content": "为了确保内容全面且符合您的要求，请确认以下几点：",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        40,
                        463,
                        551,
                        497
                    ],
                    "lines": [
                        {
                            "bbox": [
                                40,
                                463,
                                551,
                                497
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        40,
                                        463,
                                        551,
                                        497
                                    ],
                                    "score": 1.0,
                                    "content": "1. 深度：您希望对技术细节的讨论深入到论文级别（如具体的架构优化、训练方法）还是更偏向 科普性（如关键技术点的原理和应用）？",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        511,
                        502,
                        524
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                511,
                                502,
                                524
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        511,
                                        502,
                                        524
                                    ],
                                    "score": 1.0,
                                    "content": "2. 格式：您是否有偏好的格式？比如是否希望以章节划分，或者希望以问答形式呈现？",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        539,
                        555,
                        572
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                539,
                                555,
                                572
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        539,
                                        555,
                                        572
                                    ],
                                    "score": 1.0,
                                    "content": "3. benchmark 详尽度：是否需要具体的数值比较（如模型在不同任务上的具体分数），还是只需 介绍这些benchmark的作用和测试方法？",
                                    "type": "text"
                                }
                            ],
                            "index": 13
                        }
                    ],
                    "index": 13,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        587,
                        490,
                        599
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                587,
                                490,
                                599
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        587,
                                        490,
                                        599
                                    ],
                                    "score": 1.0,
                                    "content": "4. 语言：您希望整体用中文输出，但是否希望附带关键术语的英文原文，以便理解？",
                                    "type": "text"
                                }
                            ],
                            "index": 14
                        }
                    ],
                    "index": 14,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        614,
                        525,
                        626
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                614,
                                525,
                                626
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        614,
                                        525,
                                        626
                                    ],
                                    "score": 1.0,
                                    "content": "5. 目标读者：这个报告是面向一般公众，还是技术从业者，如机器学习工程师和研究人员？",
                                    "type": "text"
                                }
                            ],
                            "index": 15
                        }
                    ],
                    "index": 15,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        650,
                        292,
                        662
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                650,
                                292,
                                662
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        650,
                                        292,
                                        662
                                    ],
                                    "score": 1.0,
                                    "content": "请确认或补充您的需求，以便更精准地提供报告。",
                                    "type": "text"
                                }
                            ],
                            "index": 16
                        }
                    ],
                    "index": 16,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        688,
                        543,
                        737
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                688,
                                543,
                                737
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        688,
                                        543,
                                        737
                                    ],
                                    "score": 1.0,
                                    "content": "深度：您希望对技术细节的讨论深入到论文级别（如具体的架构 优化、训练方法）还是更偏向科普性（如关键技术点的原理和应 用）？",
                                    "type": "text"
                                }
                            ],
                            "index": 17
                        }
                    ],
                    "index": 17,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        206,
                        743,
                        291,
                        755
                    ],
                    "lines": [
                        {
                            "bbox": [
                                206,
                                743,
                                291,
                                755
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        206,
                                        743,
                                        291,
                                        755
                                    ],
                                    "score": 1.0,
                                    "content": "偏向科普性即可",
                                    "type": "text"
                                }
                            ],
                            "index": 18
                        }
                    ],
                    "index": 18,
                    "page_num": "page_0",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        205,
                        19,
                        543,
                        68
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                19,
                                543,
                                68
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        19,
                                        543,
                                        68
                                    ],
                                    "score": 1.0,
                                    "content": "格式：您是否有偏好的格式？比如是否希望以章节划分，或者希 望以问答形式呈现？ 完整的章节划分",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        91,
                        542,
                        142
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                91,
                                542,
                                142
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        91,
                                        542,
                                        142
                                    ],
                                    "score": 1.0,
                                    "content": "benchmark 详尽度：是否需要具体的数值比较（如模型在不同 任务上的具体分数），还是只需介绍这些benchmark的作用和测 试方法？",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        181,
                        543,
                        230
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                181,
                                543,
                                230
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        181,
                                        543,
                                        230
                                    ],
                                    "score": 1.0,
                                    "content": "语言：您希望整体用中文输出，但是否希望附带关键术语的英文 原文，以便理解？ 可以附带关键术语的英文原文",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        253,
                        543,
                        301
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                253,
                                543,
                                301
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        253,
                                        543,
                                        301
                                    ],
                                    "score": 1.0,
                                    "content": "目标读者：这个报告是面向一般公众，还是技术从业者，如机器 学习工程师和研究人员？ 面向一般公众",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        318,
                        557,
                        394
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                318,
                                557,
                                394
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        318,
                                        557,
                                        394
                                    ],
                                    "score": 1.0,
                                    "content": "好的，我将撰写一份详细的大语言模型科普报告，涵盖其发展历史、基础概念、benchmark 评价指 标、技术细节以及2024年及以后发布的主流大模型。报告将以完整的章节划分，偏向科普性，不涉 及论文级的深度细节，并附带关键术语的英文原文，方便理解。我会确保内容适合一般公众阅读， 同时提供清晰的解释和易懂的表达。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        408,
                        196,
                        420
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                408,
                                196,
                                420
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        408,
                                        196,
                                        420
                                    ],
                                    "score": 1.0,
                                    "content": "完成后，我会通知您进行审核。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        474,
                        406,
                        501
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                474,
                                406,
                                501
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        474,
                                        406,
                                        501
                                    ],
                                    "score": 1.0,
                                    "content": "大语言模型的发展史和基本原理",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        538,
                        550,
                        571
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                538,
                                550,
                                571
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        538,
                                        550,
                                        571
                                    ],
                                    "score": 1.0,
                                    "content": "发展历程：大语言模型（LLM, Large Language Model）的兴起可以追溯到早期的语言模型研究。 最初的语言模型使用统计方法，如1990 年代IBM 的统计翻译模型和n\\-gram 模型等",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        587,
                        559,
                        682
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                587,
                                559,
                                682
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        587,
                                        559,
                                        682
                                    ],
                                    "score": 1.0,
                                    "content": "EN.WIKIPEDIA.ORG。随着神经网络的发展，语言模型逐渐采用神经网络架构。2017 年谷歌在 NeurIPS 大 会上提出了革命性的Transformer 架构 (Transformer architecture) 。Transformer 利用自注意力机制 (Self-Attention) 高效建模长距离依赖，大幅提升了模型对长文本的处理能力。从 此，NLP 领域进入了基于Transformer 的预训练模型时代。2018 年出现了 BERT（一种仅含编码 器的双向Transformer 模型），以遮蔽预测方式训练，迅速成为自然语言理解任务的基础",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        691,
                        557,
                        767
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                691,
                                557,
                                767
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        691,
                                        557,
                                        767
                                    ],
                                    "score": 1.0,
                                    "content": "EN.WIKIPEDIA.ORG 。同年，OpenAI 发布了GPT\\-1，采用仅解码器的自回归Transformer实现语言生成。 2019 年的GPT\\-2 因其生成文本的能力过于逼真，一度被OpenAI 认为可能被滥用而暂缓公开全部 模型 EN.WIKIPEDIA.ORG 。2020 年的GPT\\-3 提升到1750亿参数，只提供API服务，被视为首批真正具备通 用生成能力的大模型 EN.WIKIPEDIA.ORG 。随后，2022年底的ChatGPT（基于GPT\\-3.5）通过人与反馈强",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                }
            ],
            "layout_bboxes": [],
            "page_idx": 1,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        205,
                        19,
                        543,
                        68
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                19,
                                543,
                                68
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        19,
                                        543,
                                        68
                                    ],
                                    "score": 1.0,
                                    "content": "格式：您是否有偏好的格式？比如是否希望以章节划分，或者希 望以问答形式呈现？ 完整的章节划分",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        91,
                        542,
                        142
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                91,
                                542,
                                142
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        91,
                                        542,
                                        142
                                    ],
                                    "score": 1.0,
                                    "content": "benchmark 详尽度：是否需要具体的数值比较（如模型在不同 任务上的具体分数），还是只需介绍这些benchmark的作用和测 试方法？",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        181,
                        543,
                        230
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                181,
                                543,
                                230
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        181,
                                        543,
                                        230
                                    ],
                                    "score": 1.0,
                                    "content": "语言：您希望整体用中文输出，但是否希望附带关键术语的英文 原文，以便理解？ 可以附带关键术语的英文原文",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        205,
                        253,
                        543,
                        301
                    ],
                    "lines": [
                        {
                            "bbox": [
                                205,
                                253,
                                543,
                                301
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        205,
                                        253,
                                        543,
                                        301
                                    ],
                                    "score": 1.0,
                                    "content": "目标读者：这个报告是面向一般公众，还是技术从业者，如机器 学习工程师和研究人员？ 面向一般公众",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        318,
                        557,
                        394
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                318,
                                557,
                                394
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        318,
                                        557,
                                        394
                                    ],
                                    "score": 1.0,
                                    "content": "好的，我将撰写一份详细的大语言模型科普报告，涵盖其发展历史、基础概念、benchmark 评价指 标、技术细节以及2024年及以后发布的主流大模型。报告将以完整的章节划分，偏向科普性，不涉 及论文级的深度细节，并附带关键术语的英文原文，方便理解。我会确保内容适合一般公众阅读， 同时提供清晰的解释和易懂的表达。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        408,
                        196,
                        420
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                408,
                                196,
                                420
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        408,
                                        196,
                                        420
                                    ],
                                    "score": 1.0,
                                    "content": "完成后，我会通知您进行审核。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        474,
                        406,
                        501
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                474,
                                406,
                                501
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        474,
                                        406,
                                        501
                                    ],
                                    "score": 1.0,
                                    "content": "大语言模型的发展史和基本原理",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        538,
                        550,
                        571
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                538,
                                550,
                                571
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        538,
                                        550,
                                        571
                                    ],
                                    "score": 1.0,
                                    "content": "发展历程：大语言模型（LLM, Large Language Model）的兴起可以追溯到早期的语言模型研究。 最初的语言模型使用统计方法，如1990 年代IBM 的统计翻译模型和n\\-gram 模型等",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        587,
                        559,
                        682
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                587,
                                559,
                                682
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        587,
                                        559,
                                        682
                                    ],
                                    "score": 1.0,
                                    "content": "EN.WIKIPEDIA.ORG。随着神经网络的发展，语言模型逐渐采用神经网络架构。2017 年谷歌在 NeurIPS 大 会上提出了革命性的Transformer 架构 (Transformer architecture) 。Transformer 利用自注意力机制 (Self-Attention) 高效建模长距离依赖，大幅提升了模型对长文本的处理能力。从 此，NLP 领域进入了基于Transformer 的预训练模型时代。2018 年出现了 BERT（一种仅含编码 器的双向Transformer 模型），以遮蔽预测方式训练，迅速成为自然语言理解任务的基础",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        691,
                        557,
                        767
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                691,
                                557,
                                767
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        691,
                                        557,
                                        767
                                    ],
                                    "score": 1.0,
                                    "content": "EN.WIKIPEDIA.ORG 。同年，OpenAI 发布了GPT\\-1，采用仅解码器的自回归Transformer实现语言生成。 2019 年的GPT\\-2 因其生成文本的能力过于逼真，一度被OpenAI 认为可能被滥用而暂缓公开全部 模型 EN.WIKIPEDIA.ORG 。2020 年的GPT\\-3 提升到1750亿参数，只提供API服务，被视为首批真正具备通 用生成能力的大模型 EN.WIKIPEDIA.ORG 。随后，2022年底的ChatGPT（基于GPT\\-3.5）通过人与反馈强",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_1",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        32,
                        21,
                        552,
                        138
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                21,
                                552,
                                138
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        21,
                                        552,
                                        138
                                    ],
                                    "score": 1.0,
                                    "content": "化学习（RLHF）微调，实现了令人惊艳的对话能力，引发大众关注。2023 年 GPT-4 发布，改进了 准确性并支持图像输入，多模态能力被誉为“圣杯”式突破EN.WIKIPEDIA.ORG 。此后，大模型领域百花齐 放，开放和闭源模型齐头并进。2024 年 OpenAI 推出GPT\\-4o (“omni”，全模态GPT\\-4)，这是一个 多语种、多模态模型，支持实时处理文本、图像和音频输入，被广泛应用于ChatGPT 等产品 EN.WIKIPEDIA.ORG 。各大公司和研究组织也相继发布更强大的模型（详见后文），标志着大 语言模型进入全民参与、快速迭代的新阶段。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        158,
                        559,
                        234
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                158,
                                559,
                                234
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        158,
                                        559,
                                        234
                                    ],
                                    "score": 1.0,
                                    "content": "基本原理：大语言模型通过神经网络(Neural Network)来处理和生成语言。现代LLM通常基于Transformer架构，由编码器和解码器堆叠组成。Transformer的核心在于注意力机制(Attention mechanism)，使模型在处理每个单词时都能参考序列中其他相关位置的内容。特别是自注意力机制让模型在预训练时学会词语之间的关联。LLM的训练通常分两步：首先进行预训练(Pre-",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        242,
                        557,
                        297
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                242,
                                557,
                                297
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        242,
                                        557,
                                        297
                                    ],
                                    "score": 1.0,
                                    "content": "training), 即在海量文本语料上进行自监督学习——模型通过自回归 (Autoregressive)方式预测下一个词（如GPT系列）或通过掩蔽语言模型 (Masked LM)方式预测被遮蔽的词（如BERT）。预训练使模型学会语言的基本语法和常识知识",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        312,
                        555,
                        366
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                312,
                                555,
                                366
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        312,
                                        555,
                                        366
                                    ],
                                    "score": 1.0,
                                    "content": "接着进行微调(Fine-tuning)，即在特定任务或指令数据上进一步训练模 型，使其适应实际应用。这包括传统的有监督微调，以及近年来常用的\\* \\* 指令微调(Instruction tuning)和人类反馈强化学习(RLHF)等方法，以提升模型跟随指令和对齐人类期望的能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        374,
                        554,
                        408
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                374,
                                554,
                                408
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        374,
                                        554,
                                        408
                                    ],
                                    "score": 1.0,
                                    "content": "Transformer模型通过堆叠多层前馈网络 (Feed-forward network)和多头注意力模块来逐步提取复杂的语言模式，参数量通常以亿计甚至千亿计（因此称为“大”模型）。模型通过反向传播",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        417,
                        549,
                        492
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                417,
                                549,
                                492
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        417,
                                        549,
                                        492
                                    ],
                                    "score": 1.0,
                                    "content": "(Backpropagation)和梯度下降 (Gradient Descent)***优化海量参数，使预测结果与训练目标（下一个词或被遮蔽词）更接近，从而逐步学会语言。总结来说，大语言模型利用深层神经网络和Transformer架构，从海量文本中自我学习语言规律，再通过微调掌握特定任务，在推理时根据输入上下文生成连贯的文本回答。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        523,
                        143,
                        540
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                523,
                                143,
                                540
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        523,
                                        143,
                                        540
                                    ],
                                    "score": 1.0,
                                    "content": "基础概念解释",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        566,
                        558,
                        599
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                566,
                                558,
                                599
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        566,
                                        558,
                                        599
                                    ],
                                    "score": 1.0,
                                    "content": "大语言模型涉及许多概念。下面以通俗方式解释至少 80 个相关基础术语，每个术语附有中文解释和英文术语：",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        626,
                        170,
                        641
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                626,
                                170,
                                641
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        626,
                                        170,
                                        641
                                    ],
                                    "score": 1.0,
                                    "content": "模型与架构相关概念",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        656,
                        553,
                        690
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                656,
                                553,
                                690
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        656,
                                        553,
                                        690
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 大语言模型(Large Language Model, LLM)：参数规模巨大（数亿到千亿以上）、在海量语料 上训练的语言模型，能够执行文本理解和生成等多种任务。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        705,
                        548,
                        737
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                705,
                                548,
                                737
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        705,
                                        548,
                                        737
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 神经网络(Neural Network)：由模拟生物神经元的节点构成的模型，通过层与层之间的加权 连接来学习数据特征。深度神经网络是大语言模型的基础。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10
                }
            ],
            "layout_bboxes": [],
            "page_idx": 2,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        32,
                        21,
                        552,
                        138
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                21,
                                552,
                                138
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        21,
                                        552,
                                        138
                                    ],
                                    "score": 1.0,
                                    "content": "化学习（RLHF）微调，实现了令人惊艳的对话能力，引发大众关注。2023 年 GPT-4 发布，改进了 准确性并支持图像输入，多模态能力被誉为“圣杯”式突破EN.WIKIPEDIA.ORG 。此后，大模型领域百花齐 放，开放和闭源模型齐头并进。2024 年 OpenAI 推出GPT\\-4o (“omni”，全模态GPT\\-4)，这是一个 多语种、多模态模型，支持实时处理文本、图像和音频输入，被广泛应用于ChatGPT 等产品 EN.WIKIPEDIA.ORG 。各大公司和研究组织也相继发布更强大的模型（详见后文），标志着大 语言模型进入全民参与、快速迭代的新阶段。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        158,
                        559,
                        234
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                158,
                                559,
                                234
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        158,
                                        559,
                                        234
                                    ],
                                    "score": 1.0,
                                    "content": "基本原理：大语言模型通过神经网络(Neural Network)来处理和生成语言。现代LLM通常基于Transformer架构，由编码器和解码器堆叠组成。Transformer的核心在于注意力机制(Attention mechanism)，使模型在处理每个单词时都能参考序列中其他相关位置的内容。特别是自注意力机制让模型在预训练时学会词语之间的关联。LLM的训练通常分两步：首先进行预训练(Pre-",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        242,
                        557,
                        297
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                242,
                                557,
                                297
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        242,
                                        557,
                                        297
                                    ],
                                    "score": 1.0,
                                    "content": "training), 即在海量文本语料上进行自监督学习——模型通过自回归 (Autoregressive)方式预测下一个词（如GPT系列）或通过掩蔽语言模型 (Masked LM)方式预测被遮蔽的词（如BERT）。预训练使模型学会语言的基本语法和常识知识",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        312,
                        555,
                        366
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                312,
                                555,
                                366
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        312,
                                        555,
                                        366
                                    ],
                                    "score": 1.0,
                                    "content": "接着进行微调(Fine-tuning)，即在特定任务或指令数据上进一步训练模 型，使其适应实际应用。这包括传统的有监督微调，以及近年来常用的\\* \\* 指令微调(Instruction tuning)和人类反馈强化学习(RLHF)等方法，以提升模型跟随指令和对齐人类期望的能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        374,
                        554,
                        408
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                374,
                                554,
                                408
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        374,
                                        554,
                                        408
                                    ],
                                    "score": 1.0,
                                    "content": "Transformer模型通过堆叠多层前馈网络 (Feed-forward network)和多头注意力模块来逐步提取复杂的语言模式，参数量通常以亿计甚至千亿计（因此称为“大”模型）。模型通过反向传播",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        417,
                        549,
                        492
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                417,
                                549,
                                492
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        417,
                                        549,
                                        492
                                    ],
                                    "score": 1.0,
                                    "content": "(Backpropagation)和梯度下降 (Gradient Descent)***优化海量参数，使预测结果与训练目标（下一个词或被遮蔽词）更接近，从而逐步学会语言。总结来说，大语言模型利用深层神经网络和Transformer架构，从海量文本中自我学习语言规律，再通过微调掌握特定任务，在推理时根据输入上下文生成连贯的文本回答。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        523,
                        143,
                        540
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                523,
                                143,
                                540
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        523,
                                        143,
                                        540
                                    ],
                                    "score": 1.0,
                                    "content": "基础概念解释",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        566,
                        558,
                        599
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                566,
                                558,
                                599
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        566,
                                        558,
                                        599
                                    ],
                                    "score": 1.0,
                                    "content": "大语言模型涉及许多概念。下面以通俗方式解释至少 80 个相关基础术语，每个术语附有中文解释和英文术语：",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        626,
                        170,
                        641
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                626,
                                170,
                                641
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        626,
                                        170,
                                        641
                                    ],
                                    "score": 1.0,
                                    "content": "模型与架构相关概念",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        656,
                        553,
                        690
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                656,
                                553,
                                690
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        656,
                                        553,
                                        690
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 大语言模型(Large Language Model, LLM)：参数规模巨大（数亿到千亿以上）、在海量语料 上训练的语言模型，能够执行文本理解和生成等多种任务。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        705,
                        548,
                        737
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                705,
                                548,
                                737
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        705,
                                        548,
                                        737
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 神经网络(Neural Network)：由模拟生物神经元的节点构成的模型，通过层与层之间的加权 连接来学习数据特征。深度神经网络是大语言模型的基础。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10,
                    "page_num": "page_2",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        39,
                        20,
                        546,
                        54
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                20,
                                546,
                                54
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        20,
                                        546,
                                        54
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 参数 (Parameters)：指模型中可学习的权重值。LLM往往拥有亿亿级别的参数，参数数量越 多，模型容量（capacity）越大，潜在表达能力越强。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        69,
                        558,
                        123
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                69,
                                558,
                                123
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        69,
                                        558,
                                        123
                                    ],
                                    "score": 1.0,
                                    "content": "- Transformer 架构 (Transformer architecture)：一种基于自注意力机制的神经网络架构，由 编码器和解码器堆叠组成。Transformer能够并行处理序列，在NLP中取代了循环神经网络， 是现代大模型的核心架构EN.WIKIPEDIA.ORG。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        137,
                        557,
                        170
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                137,
                                557,
                                170
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        137,
                                        557,
                                        170
                                    ],
                                    "score": 1.0,
                                    "content": "注意力机制(Attention mechanism)：模型在处理序列时分配给每个位置不同的权重，以关注 更相关的部分。通过“注意”输入中重要的信息，模型能更好地理解上下文。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        186,
                        556,
                        219
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                186,
                                556,
                                219
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        186,
                                        556,
                                        219
                                    ],
                                    "score": 1.0,
                                    "content": "- 自注意力机制 (Self-Attention)：Transformer中的关键机制，每个词对序列中其他词计算相关性，并根据相关性加权汇总信息，从而捕捉句子内部各词之间的联系。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        233,
                        550,
                        266
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                233,
                                550,
                                266
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        233,
                                        550,
                                        266
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 多头注意力 (Multi-head Attention)：将注意力机制复制多个“头”，每个头在不同子空间学习 不同的关系，然后将结果融合。这使模型能同时关注不同角度的语义模式。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        282,
                        556,
                        314
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                282,
                                556,
                                314
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        282,
                                        556,
                                        314
                                    ],
                                    "score": 1.0,
                                    "content": "- 前馈网络 (Feed-forward Neural Network)：Transformer中每个注意力层后面的全连接网络 层，对注意力提取的信息做进一步非线性变换和特征提取。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        330,
                        548,
                        362
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                330,
                                548,
                                362
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        330,
                                        548,
                                        362
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 编码器 (Encoder)：Transformer架构的一部分，接收输入序列并提取特征表示。编码器通常 用于理解类任务（如BERT）中，产生输入的隐藏表示。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        377,
                        550,
                        411
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                377,
                                550,
                                411
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        377,
                                        550,
                                        411
                                    ],
                                    "score": 1.0,
                                    "content": "解码器(Decoder)：Transformer的另一部分，根据先前输出和编码器提供的上下文逐步生成 序列。用于生成类任务（如GPT）中，一般通过自回归方式生成文本。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        425,
                        551,
                        479
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                425,
                                551,
                                479
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        425,
                                        551,
                                        479
                                    ],
                                    "score": 1.0,
                                    "content": "- 编码器-解码器模型 (Encoder-Decoder model)：同时包含编码器和解码器的模型架构。编码器读入源序列，解码器生成目标序列，多用于翻译等序列到序列任务（如Transformer原论文模型）。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        494,
                        558,
                        528
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                494,
                                558,
                                528
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        494,
                                        558,
                                        528
                                    ],
                                    "score": 1.0,
                                    "content": "- 自回归模型(Autoregressive model)：按照序列顺序逐词生成下一个词的模型。模型基于已经 生成的前文来预测下一个token，GPT系列属于自回归生成模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        542,
                        553,
                        575
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                542,
                                553,
                                575
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        542,
                                        553,
                                        575
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 掩蔽语言模型 (Masked Language Model)：通过遮蔽部分文本让模型预测被遮蔽内容的训练 方法。BERT采用这种方式训练，使模型学会双向理解上下文，但不直接用于生成。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        598,
                        171,
                        614
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                598,
                                171,
                                614
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        598,
                                        171,
                                        614
                                    ],
                                    "score": 1.0,
                                    "content": "训练与优化相关概念",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        629,
                        549,
                        663
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                629,
                                549,
                                663
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        629,
                                        549,
                                        663
                                    ],
                                    "score": 1.0,
                                    "content": "- 预训练(Pre-training)：在大规模未标注语料上训练模型以学习通用语言知识的过程。通过预 训练，模型获得广泛的语义、语法常识，为下游任务打下基础。",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        677,
                        557,
                        711
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                677,
                                557,
                                711
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        677,
                                        557,
                                        711
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 微调(Fine-tuning)：在预训练模型的基础上，使用较小的特定任务数据继续训练模型，使其适 应特定任务需求。例如用问答数据微调预训练模型以提升问答性能。",
                                    "type": "text"
                                }
                            ],
                            "index": 13
                        }
                    ],
                    "index": 13
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        726,
                        552,
                        781
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                726,
                                552,
                                781
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        726,
                                        552,
                                        781
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 自监督学习(Self-supervised Learning)：利用数据自身的隐藏结构作为监督信号的学习方 式。预训练时通常采用自监督，如通过预测下一个词或被遮蔽词来训练模型，无需人工标注标 签 DATABRICKS.COM 。",
                                    "type": "text"
                                }
                            ],
                            "index": 14
                        }
                    ],
                    "index": 14
                }
            ],
            "layout_bboxes": [],
            "page_idx": 3,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        39,
                        20,
                        546,
                        54
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                20,
                                546,
                                54
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        20,
                                        546,
                                        54
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 参数 (Parameters)：指模型中可学习的权重值。LLM往往拥有亿亿级别的参数，参数数量越 多，模型容量（capacity）越大，潜在表达能力越强。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        69,
                        558,
                        123
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                69,
                                558,
                                123
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        69,
                                        558,
                                        123
                                    ],
                                    "score": 1.0,
                                    "content": "- Transformer 架构 (Transformer architecture)：一种基于自注意力机制的神经网络架构，由 编码器和解码器堆叠组成。Transformer能够并行处理序列，在NLP中取代了循环神经网络， 是现代大模型的核心架构EN.WIKIPEDIA.ORG。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        137,
                        557,
                        170
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                137,
                                557,
                                170
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        137,
                                        557,
                                        170
                                    ],
                                    "score": 1.0,
                                    "content": "注意力机制(Attention mechanism)：模型在处理序列时分配给每个位置不同的权重，以关注 更相关的部分。通过“注意”输入中重要的信息，模型能更好地理解上下文。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        186,
                        556,
                        219
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                186,
                                556,
                                219
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        186,
                                        556,
                                        219
                                    ],
                                    "score": 1.0,
                                    "content": "- 自注意力机制 (Self-Attention)：Transformer中的关键机制，每个词对序列中其他词计算相关性，并根据相关性加权汇总信息，从而捕捉句子内部各词之间的联系。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        233,
                        550,
                        266
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                233,
                                550,
                                266
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        233,
                                        550,
                                        266
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 多头注意力 (Multi-head Attention)：将注意力机制复制多个“头”，每个头在不同子空间学习 不同的关系，然后将结果融合。这使模型能同时关注不同角度的语义模式。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        282,
                        556,
                        314
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                282,
                                556,
                                314
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        282,
                                        556,
                                        314
                                    ],
                                    "score": 1.0,
                                    "content": "- 前馈网络 (Feed-forward Neural Network)：Transformer中每个注意力层后面的全连接网络 层，对注意力提取的信息做进一步非线性变换和特征提取。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        330,
                        548,
                        362
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                330,
                                548,
                                362
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        330,
                                        548,
                                        362
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 编码器 (Encoder)：Transformer架构的一部分，接收输入序列并提取特征表示。编码器通常 用于理解类任务（如BERT）中，产生输入的隐藏表示。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        377,
                        550,
                        411
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                377,
                                550,
                                411
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        377,
                                        550,
                                        411
                                    ],
                                    "score": 1.0,
                                    "content": "解码器(Decoder)：Transformer的另一部分，根据先前输出和编码器提供的上下文逐步生成 序列。用于生成类任务（如GPT）中，一般通过自回归方式生成文本。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        425,
                        551,
                        479
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                425,
                                551,
                                479
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        425,
                                        551,
                                        479
                                    ],
                                    "score": 1.0,
                                    "content": "- 编码器-解码器模型 (Encoder-Decoder model)：同时包含编码器和解码器的模型架构。编码器读入源序列，解码器生成目标序列，多用于翻译等序列到序列任务（如Transformer原论文模型）。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        494,
                        558,
                        528
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                494,
                                558,
                                528
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        494,
                                        558,
                                        528
                                    ],
                                    "score": 1.0,
                                    "content": "- 自回归模型(Autoregressive model)：按照序列顺序逐词生成下一个词的模型。模型基于已经 生成的前文来预测下一个token，GPT系列属于自回归生成模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        542,
                        553,
                        575
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                542,
                                553,
                                575
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        542,
                                        553,
                                        575
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 掩蔽语言模型 (Masked Language Model)：通过遮蔽部分文本让模型预测被遮蔽内容的训练 方法。BERT采用这种方式训练，使模型学会双向理解上下文，但不直接用于生成。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        598,
                        171,
                        614
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                598,
                                171,
                                614
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        598,
                                        171,
                                        614
                                    ],
                                    "score": 1.0,
                                    "content": "训练与优化相关概念",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        629,
                        549,
                        663
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                629,
                                549,
                                663
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        629,
                                        549,
                                        663
                                    ],
                                    "score": 1.0,
                                    "content": "- 预训练(Pre-training)：在大规模未标注语料上训练模型以学习通用语言知识的过程。通过预 训练，模型获得广泛的语义、语法常识，为下游任务打下基础。",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        677,
                        557,
                        711
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                677,
                                557,
                                711
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        677,
                                        557,
                                        711
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 微调(Fine-tuning)：在预训练模型的基础上，使用较小的特定任务数据继续训练模型，使其适 应特定任务需求。例如用问答数据微调预训练模型以提升问答性能。",
                                    "type": "text"
                                }
                            ],
                            "index": 13
                        }
                    ],
                    "index": 13,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        726,
                        552,
                        781
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                726,
                                552,
                                781
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        726,
                                        552,
                                        781
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 自监督学习(Self-supervised Learning)：利用数据自身的隐藏结构作为监督信号的学习方 式。预训练时通常采用自监督，如通过预测下一个词或被遮蔽词来训练模型，无需人工标注标 签 DATABRICKS.COM 。",
                                    "type": "text"
                                }
                            ],
                            "index": 14
                        }
                    ],
                    "index": 14,
                    "page_num": "page_3",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        39,
                        21,
                        554,
                        54
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                21,
                                554,
                                54
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        21,
                                        554,
                                        54
                                    ],
                                    "score": 1.0,
                                    "content": "• 无监督学习(Unsupervised Learning)：不依赖人工标注数据的学习范式。自监督学习可视作 无监督的一种，大模型预训练典型地属于无监督学习，因为使用的是未标注的纯文本。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        68,
                        551,
                        101
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                68,
                                551,
                                101
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        68,
                                        551,
                                        101
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 监督学习(Supervised Learning)：利用带标签的数据进行训练的方式。模型从输入到输出有 明确的目标信号。微调阶段若有人工标注的数据（如问答对、翻译对等），通常采用监督学习。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        116,
                        554,
                        171
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                116,
                                554,
                                171
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        116,
                                        554,
                                        171
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 强化学习(Reinforcement Learning)：通过“奖励”和“惩罚”信号来训练智能体（模型）决策的 学习方式。在LLM中，引入RL可以让模型在没有标准答案的情况下通过试错和奖励信号自主改 进（例如训练模型自行探索推理链）。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        186,
                        553,
                        240
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                186,
                                553,
                                240
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        186,
                                        553,
                                        240
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 人类反馈强化学习 (RLHF, Reinforcement Learning from Human Feedback)：一种微调技 术，结合人类偏好来调整模型输出。训练时由人或代理对模型输出进行反馈打分，模型通过强 化学习算法（如PPO）优化，使输出更符合人类期望。这是ChatGPT成功的关键训练步骤。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        254,
                        558,
                        309
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                254,
                                558,
                                309
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        254,
                                        558,
                                        309
                                    ],
                                    "score": 1.0,
                                    "content": "- 损失函数 (Loss Function)：衡量模型输出与目标差距的函数。训练过程中模型以最小化损失函数为目标调整参数。语言模型常用交叉熵损失来衡量预测下一个词的准确程度（困惑度也是基于损失计算的一种指标）。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        323,
                        555,
                        357
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                323,
                                555,
                                357
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        323,
                                        555,
                                        357
                                    ],
                                    "score": 1.0,
                                    "content": "- 反向传播(Backpropagation)：训练神经网络的算法。通过将损失对参数的偏导数从输出层一路传播回输入层，指导每层参数的调整方向和幅度，从而逐步优化模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        372,
                        556,
                        404
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                372,
                                556,
                                404
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        372,
                                        556,
                                        404
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 梯度下降(Gradient Descent)：一种迭代优化方法。每次根据梯度信息更新模型参数，使损失 下降。大模型训练常用小批量梯度下降及其变体（如Adam优化器）高效收敛参数。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        420,
                        556,
                        474
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                420,
                                556,
                                474
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        420,
                                        556,
                                        474
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 优化器 (Optimizer)：用于调整模型参数的算法。不同优化器有不同的参数更新策略，如 SGD、Adam等。Adam是一种对梯度进行一阶二阶动量校正的优化器，在训练大模型时应用广 泛。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        488,
                        557,
                        543
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                488,
                                557,
                                543
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        488,
                                        557,
                                        543
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 训练数据(Training Data)：用于训练模型的语料或样本集合。LLM的预训练数据通常规模极其 庞大，来自网络抓取的文本（网页、书籍、维基百科等），涵盖多领域、多语言内容。数据质量 和多样性对模型能力影响很大。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        557,
                        555,
                        612
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                557,
                                555,
                                612
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        557,
                                        555,
                                        612
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 数据集 (Dataset)：指经过整理可用于训练或评测的一组数据。一些著名数据集如Wikipedia文 本、Common Crawl语料等被用于预训练。下游任务也有专门的数据集用于微调和评估（如 SQuAD问答数据集等）。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        627,
                        560,
                        660
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                627,
                                560,
                                660
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        627,
                                        560,
                                        660
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 训练轮次 (Epoch)：遍历整个训练数据集一次称为一个epoch。由于预训练语料极大，LLM的预 训练通常以数个epoch甚至不到1个epoch完成（即训练中可能并未完整看过所有语料）。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        674,
                        548,
                        708
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                674,
                                548,
                                708
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        674,
                                        548,
                                        708
                                    ],
                                    "score": 1.0,
                                    "content": "$\\bullet$ 批量(Batch)：训练时一次性送入模型并计算一次梯度更新的一组样本。批量越大，梯度估计 越稳定但显存消耗也越高。大模型训练常用分布式并行来增大等效批量。",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        731,
                        200,
                        746
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                731,
                                200,
                                746
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        731,
                                        200,
                                        746
                                    ],
                                    "score": 1.0,
                                    "content": "模型能力与行为相关概念",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12
                }
            ],
            "layout_bboxes": [],
            "page_idx": 4,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        39,
                        21,
                        554,
                        54
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                21,
                                554,
                                54
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        21,
                                        554,
                                        54
                                    ],
                                    "score": 1.0,
                                    "content": "• 无监督学习(Unsupervised Learning)：不依赖人工标注数据的学习范式。自监督学习可视作 无监督的一种，大模型预训练典型地属于无监督学习，因为使用的是未标注的纯文本。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        68,
                        551,
                        101
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                68,
                                551,
                                101
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        68,
                                        551,
                                        101
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 监督学习(Supervised Learning)：利用带标签的数据进行训练的方式。模型从输入到输出有 明确的目标信号。微调阶段若有人工标注的数据（如问答对、翻译对等），通常采用监督学习。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        116,
                        554,
                        171
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                116,
                                554,
                                171
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        116,
                                        554,
                                        171
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 强化学习(Reinforcement Learning)：通过“奖励”和“惩罚”信号来训练智能体（模型）决策的 学习方式。在LLM中，引入RL可以让模型在没有标准答案的情况下通过试错和奖励信号自主改 进（例如训练模型自行探索推理链）。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        186,
                        553,
                        240
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                186,
                                553,
                                240
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        186,
                                        553,
                                        240
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 人类反馈强化学习 (RLHF, Reinforcement Learning from Human Feedback)：一种微调技 术，结合人类偏好来调整模型输出。训练时由人或代理对模型输出进行反馈打分，模型通过强 化学习算法（如PPO）优化，使输出更符合人类期望。这是ChatGPT成功的关键训练步骤。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        254,
                        558,
                        309
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                254,
                                558,
                                309
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        254,
                                        558,
                                        309
                                    ],
                                    "score": 1.0,
                                    "content": "- 损失函数 (Loss Function)：衡量模型输出与目标差距的函数。训练过程中模型以最小化损失函数为目标调整参数。语言模型常用交叉熵损失来衡量预测下一个词的准确程度（困惑度也是基于损失计算的一种指标）。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        323,
                        555,
                        357
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                323,
                                555,
                                357
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        323,
                                        555,
                                        357
                                    ],
                                    "score": 1.0,
                                    "content": "- 反向传播(Backpropagation)：训练神经网络的算法。通过将损失对参数的偏导数从输出层一路传播回输入层，指导每层参数的调整方向和幅度，从而逐步优化模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        372,
                        556,
                        404
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                372,
                                556,
                                404
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        372,
                                        556,
                                        404
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 梯度下降(Gradient Descent)：一种迭代优化方法。每次根据梯度信息更新模型参数，使损失 下降。大模型训练常用小批量梯度下降及其变体（如Adam优化器）高效收敛参数。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        420,
                        556,
                        474
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                420,
                                556,
                                474
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        420,
                                        556,
                                        474
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 优化器 (Optimizer)：用于调整模型参数的算法。不同优化器有不同的参数更新策略，如 SGD、Adam等。Adam是一种对梯度进行一阶二阶动量校正的优化器，在训练大模型时应用广 泛。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        488,
                        557,
                        543
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                488,
                                557,
                                543
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        488,
                                        557,
                                        543
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 训练数据(Training Data)：用于训练模型的语料或样本集合。LLM的预训练数据通常规模极其 庞大，来自网络抓取的文本（网页、书籍、维基百科等），涵盖多领域、多语言内容。数据质量 和多样性对模型能力影响很大。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        557,
                        555,
                        612
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                557,
                                555,
                                612
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        557,
                                        555,
                                        612
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 数据集 (Dataset)：指经过整理可用于训练或评测的一组数据。一些著名数据集如Wikipedia文 本、Common Crawl语料等被用于预训练。下游任务也有专门的数据集用于微调和评估（如 SQuAD问答数据集等）。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        627,
                        560,
                        660
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                627,
                                560,
                                660
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        627,
                                        560,
                                        660
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 训练轮次 (Epoch)：遍历整个训练数据集一次称为一个epoch。由于预训练语料极大，LLM的预 训练通常以数个epoch甚至不到1个epoch完成（即训练中可能并未完整看过所有语料）。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        674,
                        548,
                        708
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                674,
                                548,
                                708
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        674,
                                        548,
                                        708
                                    ],
                                    "score": 1.0,
                                    "content": "$\\bullet$ 批量(Batch)：训练时一次性送入模型并计算一次梯度更新的一组样本。批量越大，梯度估计 越稳定但显存消耗也越高。大模型训练常用分布式并行来增大等效批量。",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        731,
                        200,
                        746
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                731,
                                200,
                                746
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        731,
                                        200,
                                        746
                                    ],
                                    "score": 1.0,
                                    "content": "模型能力与行为相关概念",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12,
                    "page_num": "page_4",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        39,
                        20,
                        544,
                        54
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                20,
                                544,
                                54
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        20,
                                        544,
                                        54
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 迁移学习(Transfer Learning)：将模型在一个场景/任务中学到的知识迁移到新任务的能力。 LLM通过预训练学到通用语言能力，再通过微调快速适应新任务，就是迁移学习的典型应用。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        68,
                        552,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                68,
                                552,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        68,
                                        552,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 泛化(Generalization)：模型在训练数据以外的未知输入上保持良好表现的能力。大模型参数 多样本广，泛化能力通常较强，但仍可能受训练分布影响，在陌生领域表现下降。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        116,
                        555,
                        171
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                116,
                                555,
                                171
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        116,
                                        555,
                                        171
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 过拟合(Overfitting)：模型在训练集上表现很好，但在新数据上效果变差的现象。大模型虽数 据量巨大，但若训练不当也会过拟合某些模式。通过正则化、增加数据多样性等可缓解过拟 合。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        185,
                        550,
                        240
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                185,
                                550,
                                240
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        185,
                                        550,
                                        240
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 思维链 (Chain-of-Thought, CoT)：指模型在解题时按步骤逐条推理的过程。思维链可以是模 型内部隐式进行的，也可以通过提示让模型显式输出。CoT有助于复杂推理，模拟人类逐步解 题的思路。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        255,
                        547,
                        287
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                255,
                                547,
                                287
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        255,
                                        547,
                                        287
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 零样本学习 (Zero-Shot Learning)：模型无需任何示例演示，直接根据指令完成任务的能力。 LLM常能零样本完成多种任务，因为预训练已涵盖相关模式。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        302,
                        558,
                        357
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                302,
                                558,
                                357
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        302,
                                        558,
                                        357
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 少样本学习(Few-Shot Learning)：模型仅需很少的示例（如在提示中提供1到几条范例）就能 理解新任务并产生良好结果的能力。GPT-3 展示了惊人的少样本学习能力，提示中的示例使模 型在不额外训练的情况下执行特定任务。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        371,
                        560,
                        426
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                371,
                                560,
                                426
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        371,
                                        560,
                                        426
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 上下文学习(In-Context Learning)：模型通过输入提示中的上下文（包括任务描述和示例）进 行即时学习，完成当前任务，而不改变内部权重。本质上是零样本/少样本学习，是大模型利用 提示上下文来调节输出的能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        440,
                        559,
                        495
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                440,
                                559,
                                495
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        440,
                                        559,
                                        495
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 上下文窗口 (Context Window)：模型在一次推理中能利用的文本长度范围，即模型“记忆”的对 话或文本长度。上下文窗口通常用token数量衡量，如GPT\\-3为2048 tokens，GPT\\-4可达32k， 新的模型甚至支持更长上下文。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        510,
                        552,
                        564
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                510,
                                552,
                                564
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        510,
                                        552,
                                        564
                                    ],
                                    "score": 1.0,
                                    "content": "- 多轮对话 (Multi-turn Dialogue)：模型在对话中保持上下文连续性的能力。即使经过多轮问答，模型仍能记住之前的对话内容并做出符合上下文的回答。这需要足够大的上下文窗口和对话状态管理能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        578,
                        544,
                        612
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                578,
                                544,
                                612
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        578,
                                        544,
                                        612
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 模型对齐 (Alignment)：模型的行为与人类意图和价值观保持一致。例如，不产生有害内容、 遵守用户指令等。对齐通过训练过程中的RLHF等实现，是确保模型安全可靠的重要方面。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        627,
                        556,
                        681
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                627,
                                556,
                                681
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        627,
                                        556,
                                        681
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 幻觉(Hallucination)：模型生成了看似权威但实际上虚假的信息。这是大模型常见问题，因为 模型是基于概率生成文本，可能编造不存在的事实。减少幻觉需要改进训练数据和对齐，使模 型更诚实。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        696,
                        551,
                        729
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                696,
                                551,
                                729
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        696,
                                        551,
                                        729
                                    ],
                                    "score": 1.0,
                                    "content": "偏见 (Bias)：模型由于训练数据中的偏颇而表现出的歧视性或偏见倾向。例如性别、种族偏见 等。偏见是训练数据和社会偏见的反映，需通过数据平衡和后处理来缓解。",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        744,
                        551,
                        777
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                744,
                                551,
                                777
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        744,
                                        551,
                                        777
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 安全 (Safety)：指模型避免输出有害、违法、不当言论的能力，包括避免产生仇恨言论、隐私 信息、危险指引等。为确保安全，模型需要结合内容过滤、安全指令和对齐训练等措施。",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12
                }
            ],
            "layout_bboxes": [],
            "page_idx": 5,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        39,
                        20,
                        544,
                        54
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                20,
                                544,
                                54
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        20,
                                        544,
                                        54
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 迁移学习(Transfer Learning)：将模型在一个场景/任务中学到的知识迁移到新任务的能力。 LLM通过预训练学到通用语言能力，再通过微调快速适应新任务，就是迁移学习的典型应用。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        68,
                        552,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                68,
                                552,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        68,
                                        552,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 泛化(Generalization)：模型在训练数据以外的未知输入上保持良好表现的能力。大模型参数 多样本广，泛化能力通常较强，但仍可能受训练分布影响，在陌生领域表现下降。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        116,
                        555,
                        171
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                116,
                                555,
                                171
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        116,
                                        555,
                                        171
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 过拟合(Overfitting)：模型在训练集上表现很好，但在新数据上效果变差的现象。大模型虽数 据量巨大，但若训练不当也会过拟合某些模式。通过正则化、增加数据多样性等可缓解过拟 合。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        185,
                        550,
                        240
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                185,
                                550,
                                240
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        185,
                                        550,
                                        240
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 思维链 (Chain-of-Thought, CoT)：指模型在解题时按步骤逐条推理的过程。思维链可以是模 型内部隐式进行的，也可以通过提示让模型显式输出。CoT有助于复杂推理，模拟人类逐步解 题的思路。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        255,
                        547,
                        287
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                255,
                                547,
                                287
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        255,
                                        547,
                                        287
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 零样本学习 (Zero-Shot Learning)：模型无需任何示例演示，直接根据指令完成任务的能力。 LLM常能零样本完成多种任务，因为预训练已涵盖相关模式。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        302,
                        558,
                        357
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                302,
                                558,
                                357
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        302,
                                        558,
                                        357
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 少样本学习(Few-Shot Learning)：模型仅需很少的示例（如在提示中提供1到几条范例）就能 理解新任务并产生良好结果的能力。GPT-3 展示了惊人的少样本学习能力，提示中的示例使模 型在不额外训练的情况下执行特定任务。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        371,
                        560,
                        426
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                371,
                                560,
                                426
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        371,
                                        560,
                                        426
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 上下文学习(In-Context Learning)：模型通过输入提示中的上下文（包括任务描述和示例）进 行即时学习，完成当前任务，而不改变内部权重。本质上是零样本/少样本学习，是大模型利用 提示上下文来调节输出的能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        440,
                        559,
                        495
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                440,
                                559,
                                495
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        440,
                                        559,
                                        495
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 上下文窗口 (Context Window)：模型在一次推理中能利用的文本长度范围，即模型“记忆”的对 话或文本长度。上下文窗口通常用token数量衡量，如GPT\\-3为2048 tokens，GPT\\-4可达32k， 新的模型甚至支持更长上下文。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        510,
                        552,
                        564
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                510,
                                552,
                                564
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        510,
                                        552,
                                        564
                                    ],
                                    "score": 1.0,
                                    "content": "- 多轮对话 (Multi-turn Dialogue)：模型在对话中保持上下文连续性的能力。即使经过多轮问答，模型仍能记住之前的对话内容并做出符合上下文的回答。这需要足够大的上下文窗口和对话状态管理能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        578,
                        544,
                        612
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                578,
                                544,
                                612
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        578,
                                        544,
                                        612
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 模型对齐 (Alignment)：模型的行为与人类意图和价值观保持一致。例如，不产生有害内容、 遵守用户指令等。对齐通过训练过程中的RLHF等实现，是确保模型安全可靠的重要方面。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        627,
                        556,
                        681
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                627,
                                556,
                                681
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        627,
                                        556,
                                        681
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 幻觉(Hallucination)：模型生成了看似权威但实际上虚假的信息。这是大模型常见问题，因为 模型是基于概率生成文本，可能编造不存在的事实。减少幻觉需要改进训练数据和对齐，使模 型更诚实。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        696,
                        551,
                        729
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                696,
                                551,
                                729
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        696,
                                        551,
                                        729
                                    ],
                                    "score": 1.0,
                                    "content": "偏见 (Bias)：模型由于训练数据中的偏颇而表现出的歧视性或偏见倾向。例如性别、种族偏见 等。偏见是训练数据和社会偏见的反映，需通过数据平衡和后处理来缓解。",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        744,
                        551,
                        777
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                744,
                                551,
                                777
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        744,
                                        551,
                                        777
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 安全 (Safety)：指模型避免输出有害、违法、不当言论的能力，包括避免产生仇恨言论、隐私 信息、危险指引等。为确保安全，模型需要结合内容过滤、安全指令和对齐训练等措施。",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12,
                    "page_num": "page_5",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        38,
                        21,
                        549,
                        75
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                21,
                                549,
                                75
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        21,
                                        549,
                                        75
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 涌现能力 (Emergent Abilities)：大模型在参数规模增大后意外出现的新能力。这些能力在较 小模型中不存在，但模型变“大”后突然出现，如复杂算术、多步推理等。研究发现某些任务性 能随着模型规模呈非线性提升，被称为涌现现象。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        98,
                        170,
                        113
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                98,
                                170,
                                113
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        98,
                                        170,
                                        113
                                    ],
                                    "score": 1.0,
                                    "content": "应用与技术相关概念",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        129,
                        551,
                        162
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                129,
                                551,
                                162
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        129,
                                        551,
                                        162
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 提示(Prompt)：用户输入给模型的指令或提问。有时也指包含背景、示例在内的一整段输入。 好的提示设计（Prompt Engineering）可以引导模型产生所需格式和内容的输出。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        177,
                        552,
                        231
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                177,
                                552,
                                231
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        177,
                                        552,
                                        231
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 提示工程(Prompt Engineering)：设计和优化输入提示的技巧。通过措辞、提供示例、限定 格式等方式，来最大程度发挥模型能力、控制模型输出。例如要求模型逐步推理、以特定模板 回答等。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        245,
                        558,
                        300
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                245,
                                558,
                                300
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        245,
                                        558,
                                        300
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 工具使用 (Tool Use)：指模型调用外部工具或API以完成复杂任务的能力。例如借助计算器进行 数学运算、调用搜索引擎获取实时信息等。高级LLM可以被设计成与插件集成，从而查询数据 库、执行代码等。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        314,
                        557,
                        348
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                314,
                                557,
                                348
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        314,
                                        557,
                                        348
                                    ],
                                    "score": 1.0,
                                    "content": "- 嵌入 (Embedding)：将单词或句子映射到向量空间的表示方法。模型将语言转换为连续的密集向量（embeddina）以便神经网络处理。语义相似的词在嵌入空间中距离更近。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        363,
                        558,
                        395
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                363,
                                558,
                                395
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        363,
                                        558,
                                        395
                                    ],
                                    "score": 1.0,
                                    "content": "- 嵌入空间 (Embedding Space)：由嵌入向量组成的高维向量空间。模型在该空间中表示词语或句子意义。通过训练，模型学得一个使相关概念在向量空间接近的表示空间。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        411,
                        550,
                        444
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                411,
                                550,
                                444
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        411,
                                        550,
                                        444
                                    ],
                                    "score": 1.0,
                                    "content": "• 标记(Token)：模型处理的最小文本单位，可以是一个字、一个词片段或一个符号。大语言模 型对输入进行分词(tokenization)后再处理，每个token通常对应某种子词或字符序列。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        458,
                        559,
                        492
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                458,
                                559,
                                492
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        458,
                                        559,
                                        492
                                    ],
                                    "score": 1.0,
                                    "content": "- 分词 (Tokenization)：将文本拆分成token序列的过程。常用方法如BPE（字节对编码）等，将生文本切分为模型词汇表中的基本单元。正确的分词能提升模型对罕见词的处理效率。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        507,
                        553,
                        539
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                507,
                                553,
                                539
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        507,
                                        553,
                                        539
                                    ],
                                    "score": 1.0,
                                    "content": "- 词汇表 (Vocabulary)：模型可识别的全部token集合。预训练时定义固定词汇表，包括常见词 和词片段等。输入输出都基于词汇表编码，未登录词通常拆解为更小的已知片段表示。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        554,
                        558,
                        587
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                554,
                                558,
                                587
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        554,
                                        558,
                                        587
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 模型推理 (Inference)：指使用训练好的模型生成输出的过程。给定输入后，模型经过前向计算 得到预测结果。在推理阶段模型参数冻结不变，仅用于计算。也称“推断”或“测试”阶段。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        602,
                        556,
                        656
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                602,
                                556,
                                656
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        602,
                                        556,
                                        656
                                    ],
                                    "score": 1.0,
                                    "content": "量化(Quantization)：一种模型压缩和加速技术，将模型权重从高精度（如32位浮点）降低为 低精度（如8位或4位整数）表示，以减小模型大小和提高推理速度。适当的量化可大幅减少计 算量，同时尽量保持模型性能。",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        672,
                        557,
                        726
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                672,
                                557,
                                726
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        672,
                                        557,
                                        726
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 知识蒸馏(Knowledge Distillation)：通过训练一个小模型去模仿一个大模型的输出来压缩模 型的方法。大模型作为“教师”，小模型作为“学生”。学生模型通过学习教师模型在大量输入上的 预测分布，获得接近教师的性能，但参数远少于教师模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        741,
                        556,
                        773
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                741,
                                556,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        741,
                                        556,
                                        773
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 低秩适应 (LoRA, Low-Rank Adaptation)：一种高效微调技术，冻结原模型权重，仅在每层权 重矩阵上添加小的低秩矩阵作为可训练参数 DATABRICKS.COM 。这样微调时需要训练的参数大幅减",
                                    "type": "text"
                                }
                            ],
                            "index": 13
                        }
                    ],
                    "index": 13
                }
            ],
            "layout_bboxes": [],
            "page_idx": 6,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        38,
                        21,
                        549,
                        75
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                21,
                                549,
                                75
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        21,
                                        549,
                                        75
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 涌现能力 (Emergent Abilities)：大模型在参数规模增大后意外出现的新能力。这些能力在较 小模型中不存在，但模型变“大”后突然出现，如复杂算术、多步推理等。研究发现某些任务性 能随着模型规模呈非线性提升，被称为涌现现象。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        98,
                        170,
                        113
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                98,
                                170,
                                113
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        98,
                                        170,
                                        113
                                    ],
                                    "score": 1.0,
                                    "content": "应用与技术相关概念",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        129,
                        551,
                        162
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                129,
                                551,
                                162
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        129,
                                        551,
                                        162
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 提示(Prompt)：用户输入给模型的指令或提问。有时也指包含背景、示例在内的一整段输入。 好的提示设计（Prompt Engineering）可以引导模型产生所需格式和内容的输出。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        177,
                        552,
                        231
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                177,
                                552,
                                231
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        177,
                                        552,
                                        231
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 提示工程(Prompt Engineering)：设计和优化输入提示的技巧。通过措辞、提供示例、限定 格式等方式，来最大程度发挥模型能力、控制模型输出。例如要求模型逐步推理、以特定模板 回答等。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        245,
                        558,
                        300
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                245,
                                558,
                                300
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        245,
                                        558,
                                        300
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 工具使用 (Tool Use)：指模型调用外部工具或API以完成复杂任务的能力。例如借助计算器进行 数学运算、调用搜索引擎获取实时信息等。高级LLM可以被设计成与插件集成，从而查询数据 库、执行代码等。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        314,
                        557,
                        348
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                314,
                                557,
                                348
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        314,
                                        557,
                                        348
                                    ],
                                    "score": 1.0,
                                    "content": "- 嵌入 (Embedding)：将单词或句子映射到向量空间的表示方法。模型将语言转换为连续的密集向量（embeddina）以便神经网络处理。语义相似的词在嵌入空间中距离更近。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        363,
                        558,
                        395
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                363,
                                558,
                                395
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        363,
                                        558,
                                        395
                                    ],
                                    "score": 1.0,
                                    "content": "- 嵌入空间 (Embedding Space)：由嵌入向量组成的高维向量空间。模型在该空间中表示词语或句子意义。通过训练，模型学得一个使相关概念在向量空间接近的表示空间。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        411,
                        550,
                        444
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                411,
                                550,
                                444
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        411,
                                        550,
                                        444
                                    ],
                                    "score": 1.0,
                                    "content": "• 标记(Token)：模型处理的最小文本单位，可以是一个字、一个词片段或一个符号。大语言模 型对输入进行分词(tokenization)后再处理，每个token通常对应某种子词或字符序列。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        458,
                        559,
                        492
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                458,
                                559,
                                492
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        458,
                                        559,
                                        492
                                    ],
                                    "score": 1.0,
                                    "content": "- 分词 (Tokenization)：将文本拆分成token序列的过程。常用方法如BPE（字节对编码）等，将生文本切分为模型词汇表中的基本单元。正确的分词能提升模型对罕见词的处理效率。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        507,
                        553,
                        539
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                507,
                                553,
                                539
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        507,
                                        553,
                                        539
                                    ],
                                    "score": 1.0,
                                    "content": "- 词汇表 (Vocabulary)：模型可识别的全部token集合。预训练时定义固定词汇表，包括常见词 和词片段等。输入输出都基于词汇表编码，未登录词通常拆解为更小的已知片段表示。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        554,
                        558,
                        587
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                554,
                                558,
                                587
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        554,
                                        558,
                                        587
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 模型推理 (Inference)：指使用训练好的模型生成输出的过程。给定输入后，模型经过前向计算 得到预测结果。在推理阶段模型参数冻结不变，仅用于计算。也称“推断”或“测试”阶段。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        602,
                        556,
                        656
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                602,
                                556,
                                656
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        602,
                                        556,
                                        656
                                    ],
                                    "score": 1.0,
                                    "content": "量化(Quantization)：一种模型压缩和加速技术，将模型权重从高精度（如32位浮点）降低为 低精度（如8位或4位整数）表示，以减小模型大小和提高推理速度。适当的量化可大幅减少计 算量，同时尽量保持模型性能。",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        672,
                        557,
                        726
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                672,
                                557,
                                726
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        672,
                                        557,
                                        726
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 知识蒸馏(Knowledge Distillation)：通过训练一个小模型去模仿一个大模型的输出来压缩模 型的方法。大模型作为“教师”，小模型作为“学生”。学生模型通过学习教师模型在大量输入上的 预测分布，获得接近教师的性能，但参数远少于教师模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        741,
                        556,
                        773
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                741,
                                556,
                                773
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        741,
                                        556,
                                        773
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 低秩适应 (LoRA, Low-Rank Adaptation)：一种高效微调技术，冻结原模型权重，仅在每层权 重矩阵上添加小的低秩矩阵作为可训练参数 DATABRICKS.COM 。这样微调时需要训练的参数大幅减",
                                    "type": "text"
                                }
                            ],
                            "index": 13
                        }
                    ],
                    "index": 13,
                    "page_num": "page_6",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        57,
                        21,
                        376,
                        33
                    ],
                    "lines": [
                        {
                            "bbox": [
                                57,
                                21,
                                376,
                                33
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        57,
                                        21,
                                        376,
                                        33
                                    ],
                                    "score": 1.0,
                                    "content": "少，内存占用低，易干在资源有限的环境下快速调优大模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        48,
                        557,
                        123
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                48,
                                557,
                                123
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        48,
                                        557,
                                        123
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 混合专家 (Mixture-of-Experts, MoE)：一种模型架构，将模型划分为多个“专家”子模型，每个 专家擅长不同类型的数据 IBM.COM 。输入通过一个门控网络路由到不同专家，只激活一部分专家 参与计算 IBM.COM 。这样可以在总参数量很大的情况下，每次推理只用到一小部分参数，降低计 算成本并提高模型容量。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        137,
                        558,
                        213
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                137,
                                558,
                                213
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        137,
                                        558,
                                        213
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 模拟推理(Simulated Reasoning)：一种让模型在产生最终回答前进行内部思考的技术。模型 会隐式地生成并评估中间推理步骤，再形成最后答案 $^{\\text{HELI}\\text{CONE.AI}}$ 。这类似于人类先思考再作答， 提升了模型复杂推理和规划的能力。OpenAI的o3模型采用了模拟推理机制，实现私有的“链式 思考”过程，从而在解决复杂问题时表现更好 $^{\\text{HELI}\\text{CONE.AI}}$ 。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        227,
                        552,
                        282
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                227,
                                552,
                                282
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        227,
                                        552,
                                        282
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 多模态(Multimodal)：支持多种模态输入/输出的模型。传统LLM只处理文本，而多模态模型 还能处理图像、音频等。例如GPT-4o可以接收图像和音频输入，，Claude 3.5拥有 视觉能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        296,
                        552,
                        351
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                296,
                                552,
                                351
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        296,
                                        552,
                                        351
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 多语言(Multilingual)：支持多种语言的模型。在预训练语料含多语言时，LLM可掌握多语言 能力。GPT-4o支持50多种语言 、Llama系列也提供多语言模型。这使模型能够跨 语言交流和翻译。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        366,
                        550,
                        398
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                366,
                                550,
                                398
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        366,
                                        550,
                                        398
                                    ],
                                    "score": 1.0,
                                    "content": "- 问答(Question Answering)：让模型根据给定的问题和背景材料，生成准确答案的任务。问 答是LLM常见应用之一，许多基准（如SOuAD、自然问答等）用于评测模型的问答能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        413,
                        558,
                        447
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                413,
                                558,
                                447
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        413,
                                        558,
                                        447
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 文本摘要(Text Summarization)：将一段长文本压缩为短摘要的任务。LLM通过理解文本主旨 并生成简明扼要的总结，实现自动摘要。这要求模型抓住关键信息并用简洁语言表达。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        462,
                        552,
                        516
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                462,
                                552,
                                516
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        462,
                                        552,
                                        516
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 机器翻译 (Machine Translation)：将一种语言的文本翻译成另一种语言。大型预训练模型在 双语语料上微调后，可执行高质量的自动翻译，甚至在零样本下完成跨语言翻译（依赖模型的 多语言能力）。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        530,
                        549,
                        585
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                530,
                                549,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        530,
                                        549,
                                        585
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 代码生成 (Code Generation)：根据自然语言描述自动生成源代码的任务。LLM（如OpenAI Codex、Code Llama等）能将问题描述转换成可执行的代码。HumanEval等基准专门用于评 测代码生成正确率。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        600,
                        558,
                        632
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                600,
                                558,
                                632
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        600,
                                        558,
                                        632
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 对话模型(Conversational Model)：经过特殊训练可进行对话交互的语言模型。通过在大量对 话数据和指令上微调，这类模型能够理解对话上下文、保持角色一致性并给出连贯回应。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        669,
                        549,
                        723
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                669,
                                549,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        669,
                                        549,
                                        723
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 基础模型(Foundation Model)：指在海量数据上训练的通用大模型，可适配于各种下游任务 。基础模型本身不针对特定任务，但通过微调或提示，可用于翻译、问答、对话 等多种应用。大语言模型通常被视为AI领域的基础模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        737,
                        551,
                        771
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                737,
                                551,
                                771
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        737,
                                        551,
                                        771
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 开源模型 (Open-source Model)：公开模型架构和权重，允许用户自由使用和修改的大模型。 典型如Meta的Llama系列（在许可证下开放模型权重） 、Bloom等。开源模型能",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11
                }
            ],
            "layout_bboxes": [],
            "page_idx": 7,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        57,
                        21,
                        376,
                        33
                    ],
                    "lines": [
                        {
                            "bbox": [
                                57,
                                21,
                                376,
                                33
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        57,
                                        21,
                                        376,
                                        33
                                    ],
                                    "score": 1.0,
                                    "content": "少，内存占用低，易干在资源有限的环境下快速调优大模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        48,
                        557,
                        123
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                48,
                                557,
                                123
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        48,
                                        557,
                                        123
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 混合专家 (Mixture-of-Experts, MoE)：一种模型架构，将模型划分为多个“专家”子模型，每个 专家擅长不同类型的数据 IBM.COM 。输入通过一个门控网络路由到不同专家，只激活一部分专家 参与计算 IBM.COM 。这样可以在总参数量很大的情况下，每次推理只用到一小部分参数，降低计 算成本并提高模型容量。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        137,
                        558,
                        213
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                137,
                                558,
                                213
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        137,
                                        558,
                                        213
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 模拟推理(Simulated Reasoning)：一种让模型在产生最终回答前进行内部思考的技术。模型 会隐式地生成并评估中间推理步骤，再形成最后答案 $^{\\text{HELI}\\text{CONE.AI}}$ 。这类似于人类先思考再作答， 提升了模型复杂推理和规划的能力。OpenAI的o3模型采用了模拟推理机制，实现私有的“链式 思考”过程，从而在解决复杂问题时表现更好 $^{\\text{HELI}\\text{CONE.AI}}$ 。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        227,
                        552,
                        282
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                227,
                                552,
                                282
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        227,
                                        552,
                                        282
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 多模态(Multimodal)：支持多种模态输入/输出的模型。传统LLM只处理文本，而多模态模型 还能处理图像、音频等。例如GPT-4o可以接收图像和音频输入，，Claude 3.5拥有 视觉能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        296,
                        552,
                        351
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                296,
                                552,
                                351
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        296,
                                        552,
                                        351
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 多语言(Multilingual)：支持多种语言的模型。在预训练语料含多语言时，LLM可掌握多语言 能力。GPT-4o支持50多种语言 、Llama系列也提供多语言模型。这使模型能够跨 语言交流和翻译。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        366,
                        550,
                        398
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                366,
                                550,
                                398
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        366,
                                        550,
                                        398
                                    ],
                                    "score": 1.0,
                                    "content": "- 问答(Question Answering)：让模型根据给定的问题和背景材料，生成准确答案的任务。问 答是LLM常见应用之一，许多基准（如SOuAD、自然问答等）用于评测模型的问答能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        413,
                        558,
                        447
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                413,
                                558,
                                447
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        413,
                                        558,
                                        447
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 文本摘要(Text Summarization)：将一段长文本压缩为短摘要的任务。LLM通过理解文本主旨 并生成简明扼要的总结，实现自动摘要。这要求模型抓住关键信息并用简洁语言表达。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        462,
                        552,
                        516
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                462,
                                552,
                                516
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        462,
                                        552,
                                        516
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 机器翻译 (Machine Translation)：将一种语言的文本翻译成另一种语言。大型预训练模型在 双语语料上微调后，可执行高质量的自动翻译，甚至在零样本下完成跨语言翻译（依赖模型的 多语言能力）。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        530,
                        549,
                        585
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                530,
                                549,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        530,
                                        549,
                                        585
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 代码生成 (Code Generation)：根据自然语言描述自动生成源代码的任务。LLM（如OpenAI Codex、Code Llama等）能将问题描述转换成可执行的代码。HumanEval等基准专门用于评 测代码生成正确率。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        600,
                        558,
                        632
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                600,
                                558,
                                632
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        600,
                                        558,
                                        632
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 对话模型(Conversational Model)：经过特殊训练可进行对话交互的语言模型。通过在大量对 话数据和指令上微调，这类模型能够理解对话上下文、保持角色一致性并给出连贯回应。",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        669,
                        549,
                        723
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                669,
                                549,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        669,
                                        549,
                                        723
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 基础模型(Foundation Model)：指在海量数据上训练的通用大模型，可适配于各种下游任务 。基础模型本身不针对特定任务，但通过微调或提示，可用于翻译、问答、对话 等多种应用。大语言模型通常被视为AI领域的基础模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        737,
                        551,
                        771
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                737,
                                551,
                                771
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        737,
                                        551,
                                        771
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 开源模型 (Open-source Model)：公开模型架构和权重，允许用户自由使用和修改的大模型。 典型如Meta的Llama系列（在许可证下开放模型权重） 、Bloom等。开源模型能",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11,
                    "page_num": "page_7",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        57,
                        21,
                        304,
                        33
                    ],
                    "lines": [
                        {
                            "bbox": [
                                57,
                                21,
                                304,
                                33
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        57,
                                        21,
                                        304,
                                        33
                                    ],
                                    "score": 1.0,
                                    "content": "被社区复现和改进，有助于科研和应用民主化。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        47,
                        546,
                        82
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                47,
                                546,
                                82
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        47,
                                        546,
                                        82
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 闭源模型 (Proprietary Model)：未公开细节，仅通过API或特定接口提供服务的模型。如 OpenAI的GPT\\-4、Anthropic的Claude等。闭源模型往往性能领先但使用受限，由公司控制。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        95,
                        558,
                        128
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                95,
                                558,
                                128
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        95,
                                        558,
                                        128
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 基准测试 (Benchmark)：评价模型性能的标准化测试集合。通过在公共基准上的表现，可客观 比较不同模型的能力（详见下节）。模型开发者常以各种Benchmark成绩来展示模型改进。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        143,
                        559,
                        198
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                143,
                                559,
                                198
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        143,
                                        559,
                                        198
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 通用人工智能（AGI, Artificial General Intelligence）：一种理论上的人工智能，能胜任人类能 够执行的一切智力任务。AGI通常被视为AI发展的终极目标。当前的大语言模型已在某些认知任 务上接近人类水平，但仍不是严格意义上的AGI。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        213,
                        552,
                        267
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                213,
                                552,
                                267
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        213,
                                        552,
                                        267
                                    ],
                                    "score": 1.0,
                                    "content": "• 检索增强生成 (RAG, Retrieval-Augmented Generation)：在生成文本时结合检索系统的技术。模型先根据查询从外部知识库检索相关资料，然后将资料与提示一并输入，以生成基于实时知识的准确回答。这种方法提高了答案的可靠性，减少幻觉，在问答等应用中很实用。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        282,
                        557,
                        335
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                282,
                                557,
                                335
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        282,
                                        557,
                                        335
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 多任务学习 (Multi-task Learning)：在单一模型中同时学习处理多种不同任务的能力。LLM的 预训练可看作隐式的多任务学习（学习了语言建模、知识记忆等多方面任务），而一些研究也尝 试在微调时让模型同时优化多任务目标，从而提升模型的通用性。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        351,
                        559,
                        404
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                351,
                                559,
                                404
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        351,
                                        559,
                                        404
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet GPT (Generative Pre-trained Transformer)：指OpenAI推出的生成式预训练Transformer模 型系列。GPT\\-1到GPT\\-4不断扩大参数规模和能力，其中GPT\\-3展示了惊人的零/少样本学习能 力，GPT\\-4进一步增强推理和多模态能力。GPT本身已成为强大语言模型的代名词。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        420,
                        556,
                        474
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                420,
                                556,
                                474
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        420,
                                        556,
                                        474
                                    ],
                                    "score": 1.0,
                                    "content": "• BERT (Bidirectional Encoder Representations from Transformers)：谷歌在2018年发布 的双向编码器预训练模型。BERT采用掩蔽语言模型预训练，擅长理解类任务，在问答和分类等 NLU任务上曾刷新多项纪录，是NLU领域的重要里程碑。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        489,
                        552,
                        543
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                489,
                                552,
                                543
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        489,
                                        552,
                                        543
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet ChatGPT：基于GPT系列模型的聊天机器人，由OpenAI推出。通过指令微调和RLHF，使GPT 模型能够与人进行自然对话。ChatGPT具有人性化的应答风格和强大的语言理解生成能力，引 领了大语言模型在大众应用中的热潮。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                },
                {
                    "type": "text",
                    "bbox": [
                        41,
                        573,
                        552,
                        585
                    ],
                    "lines": [
                        {
                            "bbox": [
                                41,
                                573,
                                552,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        41,
                                        573,
                                        552,
                                        585
                                    ],
                                    "score": 1.0,
                                    "content": "（以上概念涵盖了大语言模型领域的主要基础术语，帮助读者在后续内容中更好地理解相关讨论。）",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "title",
                    "bbox": [
                        34,
                        623,
                        213,
                        639
                    ],
                    "lines": [
                        {
                            "bbox": [
                                34,
                                623,
                                213,
                                639
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        34,
                                        623,
                                        213,
                                        639
                                    ],
                                    "score": 1.0,
                                    "content": "Benchmark 评价指标",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        666,
                        552,
                        720
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                666,
                                552,
                                720
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        666,
                                        552,
                                        720
                                    ],
                                    "score": 1.0,
                                    "content": "衡量大语言模型能力常依赖一系列基准测试 (Benchmarks)。这些Benchmark是预先构建的标准测试集或任务集合，用于评估模型在知识、推理、理解、生成等方面的表现。随着模型能力提升，Benchmark也在不断演进。下面介绍一些常见且最新的大语言模型Benchmark及其作用：",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        741,
                        556,
                        774
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                741,
                                556,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        741,
                                        556,
                                        774
                                    ],
                                    "score": 1.0,
                                    "content": "- GLUE/SuperGLUE：早期用于评测自然语言理解的基准集合。包含文本蕴含、问答、情感分析 等九项任务。SuperGLUE是GLUE的升级版，更加困难。LLM在这些基准上已经接近或超过人",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12
                }
            ],
            "layout_bboxes": [],
            "page_idx": 8,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        538,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                538,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        538,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        57,
                        21,
                        304,
                        33
                    ],
                    "lines": [
                        {
                            "bbox": [
                                57,
                                21,
                                304,
                                33
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        57,
                                        21,
                                        304,
                                        33
                                    ],
                                    "score": 1.0,
                                    "content": "被社区复现和改进，有助于科研和应用民主化。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        47,
                        546,
                        82
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                47,
                                546,
                                82
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        47,
                                        546,
                                        82
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 闭源模型 (Proprietary Model)：未公开细节，仅通过API或特定接口提供服务的模型。如 OpenAI的GPT\\-4、Anthropic的Claude等。闭源模型往往性能领先但使用受限，由公司控制。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        95,
                        558,
                        128
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                95,
                                558,
                                128
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        95,
                                        558,
                                        128
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 基准测试 (Benchmark)：评价模型性能的标准化测试集合。通过在公共基准上的表现，可客观 比较不同模型的能力（详见下节）。模型开发者常以各种Benchmark成绩来展示模型改进。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        143,
                        559,
                        198
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                143,
                                559,
                                198
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        143,
                                        559,
                                        198
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 通用人工智能（AGI, Artificial General Intelligence）：一种理论上的人工智能，能胜任人类能 够执行的一切智力任务。AGI通常被视为AI发展的终极目标。当前的大语言模型已在某些认知任 务上接近人类水平，但仍不是严格意义上的AGI。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        213,
                        552,
                        267
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                213,
                                552,
                                267
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        213,
                                        552,
                                        267
                                    ],
                                    "score": 1.0,
                                    "content": "• 检索增强生成 (RAG, Retrieval-Augmented Generation)：在生成文本时结合检索系统的技术。模型先根据查询从外部知识库检索相关资料，然后将资料与提示一并输入，以生成基于实时知识的准确回答。这种方法提高了答案的可靠性，减少幻觉，在问答等应用中很实用。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        282,
                        557,
                        335
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                282,
                                557,
                                335
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        282,
                                        557,
                                        335
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 多任务学习 (Multi-task Learning)：在单一模型中同时学习处理多种不同任务的能力。LLM的 预训练可看作隐式的多任务学习（学习了语言建模、知识记忆等多方面任务），而一些研究也尝 试在微调时让模型同时优化多任务目标，从而提升模型的通用性。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        351,
                        559,
                        404
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                351,
                                559,
                                404
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        351,
                                        559,
                                        404
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet GPT (Generative Pre-trained Transformer)：指OpenAI推出的生成式预训练Transformer模 型系列。GPT\\-1到GPT\\-4不断扩大参数规模和能力，其中GPT\\-3展示了惊人的零/少样本学习能 力，GPT\\-4进一步增强推理和多模态能力。GPT本身已成为强大语言模型的代名词。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        420,
                        556,
                        474
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                420,
                                556,
                                474
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        420,
                                        556,
                                        474
                                    ],
                                    "score": 1.0,
                                    "content": "• BERT (Bidirectional Encoder Representations from Transformers)：谷歌在2018年发布 的双向编码器预训练模型。BERT采用掩蔽语言模型预训练，擅长理解类任务，在问答和分类等 NLU任务上曾刷新多项纪录，是NLU领域的重要里程碑。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        489,
                        552,
                        543
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                489,
                                552,
                                543
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        489,
                                        552,
                                        543
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet ChatGPT：基于GPT系列模型的聊天机器人，由OpenAI推出。通过指令微调和RLHF，使GPT 模型能够与人进行自然对话。ChatGPT具有人性化的应答风格和强大的语言理解生成能力，引 领了大语言模型在大众应用中的热潮。",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        41,
                        573,
                        552,
                        585
                    ],
                    "lines": [
                        {
                            "bbox": [
                                41,
                                573,
                                552,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        41,
                                        573,
                                        552,
                                        585
                                    ],
                                    "score": 1.0,
                                    "content": "（以上概念涵盖了大语言模型领域的主要基础术语，帮助读者在后续内容中更好地理解相关讨论。）",
                                    "type": "text"
                                }
                            ],
                            "index": 9
                        }
                    ],
                    "index": 9,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        34,
                        623,
                        213,
                        639
                    ],
                    "lines": [
                        {
                            "bbox": [
                                34,
                                623,
                                213,
                                639
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        34,
                                        623,
                                        213,
                                        639
                                    ],
                                    "score": 1.0,
                                    "content": "Benchmark 评价指标",
                                    "type": "text"
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "index": 10,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        666,
                        552,
                        720
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                666,
                                552,
                                720
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        666,
                                        552,
                                        720
                                    ],
                                    "score": 1.0,
                                    "content": "衡量大语言模型能力常依赖一系列基准测试 (Benchmarks)。这些Benchmark是预先构建的标准测试集或任务集合，用于评估模型在知识、推理、理解、生成等方面的表现。随着模型能力提升，Benchmark也在不断演进。下面介绍一些常见且最新的大语言模型Benchmark及其作用：",
                                    "type": "text"
                                }
                            ],
                            "index": 11
                        }
                    ],
                    "index": 11,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        741,
                        556,
                        774
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                741,
                                556,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        741,
                                        556,
                                        774
                                    ],
                                    "score": 1.0,
                                    "content": "- GLUE/SuperGLUE：早期用于评测自然语言理解的基准集合。包含文本蕴含、问答、情感分析 等九项任务。SuperGLUE是GLUE的升级版，更加困难。LLM在这些基准上已经接近或超过人",
                                    "type": "text"
                                }
                            ],
                            "index": 12
                        }
                    ],
                    "index": 12,
                    "page_num": "page_8",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        57,
                        21,
                        291,
                        33
                    ],
                    "lines": [
                        {
                            "bbox": [
                                57,
                                21,
                                291,
                                33
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        57,
                                        21,
                                        291,
                                        33
                                    ],
                                    "score": 1.0,
                                    "content": "类水平，标志着模型对基础NLP任务的掌握。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        48,
                        551,
                        144
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                48,
                                551,
                                144
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        48,
                                        551,
                                        144
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet MMLU (Massive Multitask Language Understanding)：综合性知识测评基准，涵盖57个 学科的考试题，难度相当于大学本科水平。用于测试模型的广泛知识和理解能力。例如 GPT-4o在MMLU上得分88.7，高于GPT-4的86.5  。一些新模型（如Claude 3.5） 在该基准上取得领先，体现其多领域知识掌握程度。 。还有衍生的 MMLU-Pro，包 含更高难度的问题，用于进一步区分顶尖模型的知识极限 。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        159,
                        553,
                        213
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                159,
                                553,
                                213
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        159,
                                        553,
                                        213
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet BIG-bench (Beyond the Imitation Game)：开放AI社区创建的大型基准，包含约200项多样 化任务，从数学推理、常识问答到创意写作，应有尽有。用于发掘模型的长尾能力和奇特行 为。BIG-bench旨在探测模型的综合AI能力，对标通用智能。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        227,
                        557,
                        346
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                227,
                                557,
                                346
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        227,
                                        557,
                                        346
                                    ],
                                    "score": 1.0,
                                    "content": "$\\bullet$ ARC：有两个不同的基准使用该缩写：一种是Allen AI的ARC（AI2 Reasoning Challenge）， 包含中小学科学考试题，考查常识推理；另一种是François Chollet提出的“**ARC\\-**AGI” （Abstraction and Reasoning Corpus），是一套抽象图形推理任务，被认为靠近“智力测验”范 畴。后者要求模型在给定示例的基础上推理输出图形，被用于评估模型的类比和抽象推理能 力。OpenAI的o3模型在ARC\\-AGI可视化推理上达到87.5%的准确率，与人类85%的水平相当 HELICON.AI",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        359,
                        558,
                        414
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                359,
                                558,
                                414
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        359,
                                        558,
                                        414
                                    ],
                                    "score": 1.0,
                                    "content": "- HellaSwag：常识推理基准，给出不完整的句子或短文，让模型从选项中选择最合理的结尾。因为题目经过对抗过滤，“表面模式”难以奏效，需依赖常识和推理。LLM在HellaSwag上的准确率可衡量模型的常识推理能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        429,
                        558,
                        483
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                429,
                                558,
                                483
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        429,
                                        558,
                                        483
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet TruthfulQA：由817道开放问答题组成，专门测试模型回答是否真实且不受训练中常见谣言误 导DOCS CONFIDENTIAL.AI.COM 。许多模型会因为训练语料中的错误信息而给出不真实回答。TruthfulQA要 求模型避免迎合虚假或误导性说法，是评估模型真实性和抗幻觉能力的重要基准。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        497,
                        558,
                        615
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                497,
                                558,
                                615
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        497,
                                        558,
                                        615
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 数学与逻辑基准：如GSM8K（小学数学文字题）、MATH 数据集（包含中学数学竞赛题），以 及AIME 2024 等数学竞赛题库。它们用来测试模型的数学推理与计算能力。数学问题往往需要 多步推理和精确计算，是模型的一大挑战。最新模型（如DeepSeek-R1和Kimi k1.5）在这些基 准上取得突破性成绩。例如DeepSeek-R1在AIME 2024 邀请赛题目上表现出色； Kimi k1.5 在MATH\\-500 题集上达到90%以上的高准确率 AR5IV.ORG 。数学基准能够检验模型的严 谁推理和逐步计算能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        629,
                        555,
                        727
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                629,
                                555,
                                727
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        629,
                                        555,
                                        727
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 编码与编程基准：如 HumanEval 和 Codeforces。HumanEval 是一组编程题，要求模型生成 满足特定功能要求的Python代码，其通过测试用例的百分比作为得分。Claude 3.5等模型在 HumanEval上显著提高了代码正确率，显示出优异的编程能力 Anthropic.com 。Codeforces则是 竞技编程平台，通过将模型解题能力映射到相应的竞技评分来评估模型的代码能力和效率。 OpenAI的o3模型据报道在Codeforces难题上取得相当于高手的水平 Helicon AI 。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        741,
                        554,
                        774
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                741,
                                554,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        741,
                                        554,
                                        774
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet GPQA \\& GPQA Diamond：GPQA(Graduate-level Google-Proof Q\\&A) 是2023年提出的新基 准，由专业人士撰写的研究生水平生物、物理、化学选择题 ARXIV.ORG 。题目设计刻意规避网络",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8
                }
            ],
            "layout_bboxes": [],
            "page_idx": 9,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        57,
                        21,
                        291,
                        33
                    ],
                    "lines": [
                        {
                            "bbox": [
                                57,
                                21,
                                291,
                                33
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        57,
                                        21,
                                        291,
                                        33
                                    ],
                                    "score": 1.0,
                                    "content": "类水平，标志着模型对基础NLP任务的掌握。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        48,
                        551,
                        144
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                48,
                                551,
                                144
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        48,
                                        551,
                                        144
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet MMLU (Massive Multitask Language Understanding)：综合性知识测评基准，涵盖57个 学科的考试题，难度相当于大学本科水平。用于测试模型的广泛知识和理解能力。例如 GPT-4o在MMLU上得分88.7，高于GPT-4的86.5  。一些新模型（如Claude 3.5） 在该基准上取得领先，体现其多领域知识掌握程度。 。还有衍生的 MMLU-Pro，包 含更高难度的问题，用于进一步区分顶尖模型的知识极限 。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        159,
                        553,
                        213
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                159,
                                553,
                                213
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        159,
                                        553,
                                        213
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet BIG-bench (Beyond the Imitation Game)：开放AI社区创建的大型基准，包含约200项多样 化任务，从数学推理、常识问答到创意写作，应有尽有。用于发掘模型的长尾能力和奇特行 为。BIG-bench旨在探测模型的综合AI能力，对标通用智能。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        227,
                        557,
                        346
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                227,
                                557,
                                346
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        227,
                                        557,
                                        346
                                    ],
                                    "score": 1.0,
                                    "content": "$\\bullet$ ARC：有两个不同的基准使用该缩写：一种是Allen AI的ARC（AI2 Reasoning Challenge）， 包含中小学科学考试题，考查常识推理；另一种是François Chollet提出的“**ARC\\-**AGI” （Abstraction and Reasoning Corpus），是一套抽象图形推理任务，被认为靠近“智力测验”范 畴。后者要求模型在给定示例的基础上推理输出图形，被用于评估模型的类比和抽象推理能 力。OpenAI的o3模型在ARC\\-AGI可视化推理上达到87.5%的准确率，与人类85%的水平相当 HELICON.AI",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        359,
                        558,
                        414
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                359,
                                558,
                                414
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        359,
                                        558,
                                        414
                                    ],
                                    "score": 1.0,
                                    "content": "- HellaSwag：常识推理基准，给出不完整的句子或短文，让模型从选项中选择最合理的结尾。因为题目经过对抗过滤，“表面模式”难以奏效，需依赖常识和推理。LLM在HellaSwag上的准确率可衡量模型的常识推理能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        429,
                        558,
                        483
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                429,
                                558,
                                483
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        429,
                                        558,
                                        483
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet TruthfulQA：由817道开放问答题组成，专门测试模型回答是否真实且不受训练中常见谣言误 导DOCS CONFIDENTIAL.AI.COM 。许多模型会因为训练语料中的错误信息而给出不真实回答。TruthfulQA要 求模型避免迎合虚假或误导性说法，是评估模型真实性和抗幻觉能力的重要基准。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        497,
                        558,
                        615
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                497,
                                558,
                                615
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        497,
                                        558,
                                        615
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 数学与逻辑基准：如GSM8K（小学数学文字题）、MATH 数据集（包含中学数学竞赛题），以 及AIME 2024 等数学竞赛题库。它们用来测试模型的数学推理与计算能力。数学问题往往需要 多步推理和精确计算，是模型的一大挑战。最新模型（如DeepSeek-R1和Kimi k1.5）在这些基 准上取得突破性成绩。例如DeepSeek-R1在AIME 2024 邀请赛题目上表现出色； Kimi k1.5 在MATH\\-500 题集上达到90%以上的高准确率 AR5IV.ORG 。数学基准能够检验模型的严 谁推理和逐步计算能力。",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        629,
                        555,
                        727
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                629,
                                555,
                                727
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        629,
                                        555,
                                        727
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet 编码与编程基准：如 HumanEval 和 Codeforces。HumanEval 是一组编程题，要求模型生成 满足特定功能要求的Python代码，其通过测试用例的百分比作为得分。Claude 3.5等模型在 HumanEval上显著提高了代码正确率，显示出优异的编程能力 Anthropic.com 。Codeforces则是 竞技编程平台，通过将模型解题能力映射到相应的竞技评分来评估模型的代码能力和效率。 OpenAI的o3模型据报道在Codeforces难题上取得相当于高手的水平 Helicon AI 。",
                                    "type": "text"
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "index": 7,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        741,
                        554,
                        774
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                741,
                                554,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        741,
                                        554,
                                        774
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet GPQA \\& GPQA Diamond：GPQA(Graduate-level Google-Proof Q\\&A) 是2023年提出的新基 准，由专业人士撰写的研究生水平生物、物理、化学选择题 ARXIV.ORG 。题目设计刻意规避网络",
                                    "type": "text"
                                }
                            ],
                            "index": 8
                        }
                    ],
                    "index": 8,
                    "page_num": "page_9",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        56,
                        21,
                        556,
                        159
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                21,
                                556,
                                159
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        21,
                                        556,
                                        159
                                    ],
                                    "score": 1.0,
                                    "content": "直接查询，可谓“Google防不住”的难题。PhD水平专家答对率约65%，非专业人士即使上网查 资料也只有34%正确 ARXIV.ORG 。即使最先进的AI模型在GPQA也屡屡受挫——GPT\\-4的基线模型 仅39%准确 ARXIV.ORG 。其中最困难的一批问题被称为GPQA Diamond（钻石集），专门用于挑 战顶尖模型。GPQA强调深度科学推理而非简单记忆，是考验模型高层次推理和可靠性的利器 IASK.AI IASK.AI 。目前OpenAI的o1、Anthropic的Claude 3.5等顶级模型在Diamond集上的正确 率也仅略过一半，可见其难度之高（模型间具体差异此处不做比较，仅说明任务艰巨程度）。 GPQA的出现为探索“如何让AI在超越人类专家的领域仍保持可靠”提供了实验平台 ARXIV.ORG 。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        188,
                        560,
                        285
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                188,
                                560,
                                285
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        188,
                                        560,
                                        285
                                    ],
                                    "score": 1.0,
                                    "content": "上述Benchmarks各有侧重：有的考查语言理解与常识（GLUE/HELLASwag），有的评估知识广度（MMLU）、推理深度（GPQA）、数学编程能力（MATH/HumanEval）或真实性（TruthfulQA）。研究者通过在这些Benchmark上的测试，了解模型长短板，推动模型朝着更全面智能的方向改进。需要注意的是，Benchmark分数虽可比较模型优劣，但在此我们不聚焦具体分数高低，而是介绍这些评测的内容和意义。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        322,
                        143,
                        339
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                322,
                                143,
                                339
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        322,
                                        143,
                                        339
                                    ],
                                    "score": 1.0,
                                    "content": "技术细节介绍",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        366,
                        551,
                        398
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                366,
                                551,
                                398
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        366,
                                        551,
                                        398
                                    ],
                                    "score": 1.0,
                                    "content": "大语言模型在架构设计、训练方式和优化手段上有诸多技术要点。下面从科普角度介绍不同类型模 型的架构特点，以及关键技术如训练范式、参数优化和推理加速等。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        419,
                        554,
                        495
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                419,
                                554,
                                495
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        419,
                                        554,
                                        495
                                    ],
                                    "score": 1.0,
                                    "content": "开源 vs. 闭源架构：一方面，有许多开源大语言模型，其架构、训练数据甚至权重参数都公开透明，如Meta的Llama系列、Bloom、OPT等。开源模型往往由社区协作训练，具有开放许可证，用户可以下载运行、自行微调甚至修改。这促进了研究民主化，例如Meta将Llama 3免费开放给开发者使用，大大降低了使用门槛，被视为“干扰”竞争对手收费模式、推动AI普惠",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        510,
                        558,
                        710
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                510,
                                558,
                                710
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        510,
                                        558,
                                        710
                                    ],
                                    "score": 1.0,
                                    "content": "KAVOUT.COM KAVOUT.COM 。开源模型的设计思路通常追求高效和可复现，比如Llama 3使用了分层的预训 练方案，并提供多种参数规模（从数十亿到数百亿）供不同需求使用 KAVOUT.COM 。另一方面，闭源模 型由公司私有训练，如OpenAI的GPT\\-4、Anthropic的Claude系列等。闭源模型往往参数规模更 大、训练数据更丰富，在某些Benchmark上领先。但其内部结构和训练细节未公开，仅能通过API 调用。这类模型在架构上可能采用定制的改进（例如未公开的稀疏Mixture\\-of\\-Experts层，或专有 的训练策略），以追求性能最大化。开源与闭源模型在架构上的另一个区别是可扩展性：开源模型通 常支持本地部署和裁剪优化，而闭源模型通常部署在云端，由提供方进行高度优化（例如OpenAI为 GPT\\-4定制了推理集群）。总的来说，开源模型倡导透明共享，架构设计注重高效和通用；闭源模型 则倾向于大规模和商用优化，架构细节成谜但性能卓越。这两种模式共同繁荣，推动了LLM的发 展。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                }
            ],
            "layout_bboxes": [],
            "page_idx": 10,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        56,
                        21,
                        556,
                        159
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                21,
                                556,
                                159
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        21,
                                        556,
                                        159
                                    ],
                                    "score": 1.0,
                                    "content": "直接查询，可谓“Google防不住”的难题。PhD水平专家答对率约65%，非专业人士即使上网查 资料也只有34%正确 ARXIV.ORG 。即使最先进的AI模型在GPQA也屡屡受挫——GPT\\-4的基线模型 仅39%准确 ARXIV.ORG 。其中最困难的一批问题被称为GPQA Diamond（钻石集），专门用于挑 战顶尖模型。GPQA强调深度科学推理而非简单记忆，是考验模型高层次推理和可靠性的利器 IASK.AI IASK.AI 。目前OpenAI的o1、Anthropic的Claude 3.5等顶级模型在Diamond集上的正确 率也仅略过一半，可见其难度之高（模型间具体差异此处不做比较，仅说明任务艰巨程度）。 GPQA的出现为探索“如何让AI在超越人类专家的领域仍保持可靠”提供了实验平台 ARXIV.ORG 。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_10",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        33,
                        188,
                        560,
                        285
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                188,
                                560,
                                285
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        188,
                                        560,
                                        285
                                    ],
                                    "score": 1.0,
                                    "content": "上述Benchmarks各有侧重：有的考查语言理解与常识（GLUE/HELLASwag），有的评估知识广度（MMLU）、推理深度（GPQA）、数学编程能力（MATH/HumanEval）或真实性（TruthfulQA）。研究者通过在这些Benchmark上的测试，了解模型长短板，推动模型朝着更全面智能的方向改进。需要注意的是，Benchmark分数虽可比较模型优劣，但在此我们不聚焦具体分数高低，而是介绍这些评测的内容和意义。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_10",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        322,
                        143,
                        339
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                322,
                                143,
                                339
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        322,
                                        143,
                                        339
                                    ],
                                    "score": 1.0,
                                    "content": "技术细节介绍",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_10",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        366,
                        551,
                        398
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                366,
                                551,
                                398
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        366,
                                        551,
                                        398
                                    ],
                                    "score": 1.0,
                                    "content": "大语言模型在架构设计、训练方式和优化手段上有诸多技术要点。下面从科普角度介绍不同类型模 型的架构特点，以及关键技术如训练范式、参数优化和推理加速等。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_10",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        419,
                        554,
                        495
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                419,
                                554,
                                495
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        419,
                                        554,
                                        495
                                    ],
                                    "score": 1.0,
                                    "content": "开源 vs. 闭源架构：一方面，有许多开源大语言模型，其架构、训练数据甚至权重参数都公开透明，如Meta的Llama系列、Bloom、OPT等。开源模型往往由社区协作训练，具有开放许可证，用户可以下载运行、自行微调甚至修改。这促进了研究民主化，例如Meta将Llama 3免费开放给开发者使用，大大降低了使用门槛，被视为“干扰”竞争对手收费模式、推动AI普惠",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_10",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        510,
                        558,
                        710
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                510,
                                558,
                                710
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        510,
                                        558,
                                        710
                                    ],
                                    "score": 1.0,
                                    "content": "KAVOUT.COM KAVOUT.COM 。开源模型的设计思路通常追求高效和可复现，比如Llama 3使用了分层的预训 练方案，并提供多种参数规模（从数十亿到数百亿）供不同需求使用 KAVOUT.COM 。另一方面，闭源模 型由公司私有训练，如OpenAI的GPT\\-4、Anthropic的Claude系列等。闭源模型往往参数规模更 大、训练数据更丰富，在某些Benchmark上领先。但其内部结构和训练细节未公开，仅能通过API 调用。这类模型在架构上可能采用定制的改进（例如未公开的稀疏Mixture\\-of\\-Experts层，或专有 的训练策略），以追求性能最大化。开源与闭源模型在架构上的另一个区别是可扩展性：开源模型通 常支持本地部署和裁剪优化，而闭源模型通常部署在云端，由提供方进行高度优化（例如OpenAI为 GPT\\-4定制了推理集群）。总的来说，开源模型倡导透明共享，架构设计注重高效和通用；闭源模型 则倾向于大规模和商用优化，架构细节成谜但性能卓越。这两种模式共同繁荣，推动了LLM的发 展。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_10",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        32,
                        26,
                        558,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                26,
                                558,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        26,
                                        558,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "模型设计思路：除了开源/闭源之分，不同模型在设计上也各有侧重。有的模型追求更大的规模和多 样的技能，例如谷歌的Palm和后续的Gemini据称集成了强化学习、视觉\\-语言融合等多种技术，力 图打造“通才”模型。也有模型专注于推理能力的提升，例如OpenAI的o系列模型（o1、o3）强调增 强模型的逻辑思考过程，采用了模拟推理(Simulated Reasoning)等新技术",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        116,
                        556,
                        276
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                116,
                                556,
                                276
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        116,
                                        556,
                                        276
                                    ],
                                    "score": 1.0,
                                    "content": "。通过在生成答案前让模型进行内部的连贯“思考”，o系列模型在复杂问题上的正确率显 著提高 。类似地，Google的Gemini 2.0被报道引入了“Flash Thinking”机制，与模拟推 理异曲同工，旨在让模型更好地规划解题步骤 。还有一些模型在架构上引入模块化或稀疏 激活思想，例如DeepMind的Gopher曾探索不同模块协作，NVIDIA的Megatron-Turing提及专家 混合（MoE）的大规模应用。这类模型通过让不同部分专精不同任务，提升整体能力。而开源社区 中出现的如MPT等模型则注重长上下文，有针对性地将上下文窗口扩展到数十万token，以满足长 文档处理需求。总之，不同设计思路体现在：有的扩大模型规模追求广度，有的改良内部机制增强 深度推理，有的面向特定应用（如超长文本、多模态），体现了架构设计的多样化。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        296,
                        553,
                        520
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                296,
                                553,
                                520
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        296,
                                        553,
                                        520
                                    ],
                                    "score": 1.0,
                                    "content": "模型训练方式：大语言模型主要采用自监督学习进行预训练，具体可以是自回归或填空式的训练目 标（前文已介绍）。自回归预训练让模型学会语言生成（如GPT系列从海量文本中学习“下一词概 率”）；遮蔽式预训练让模型学会双向理解（如BERT通过在句中预测被遮挡词来学习上下文表示）。 无论哪种预训练方式，本质都是利用海量未标注文本，让模型学习语言模式和知识。这属于自监督 预训练范式，使模型具备通用语言能力。预训练完成后，还会进行监督微调和对齐调优。监督微调 使用人工标注的数据，例如将模型在翻译语料、问答对话等数据上训练，使其输出符合任务需求。 近年来更强调对模型进行指令调优，即使用人类编写的大量问答对话、指令执行示例来微调模型， 使其更善于遵循指示和进行对话。这一步骤让模型从“预测下一个词”转变为“完成用户指定的任务”。 在此基础上，领先的对话模型还采用RLHF（人类反馈强化学习）进一步打磨。具体做法是：先由 模型生成多个回答，人工或辅助模型对回答质量打分，然后通过强化学习（如PPO算法）调整模型 参数，使其倾向于得分高的回答",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        534,
                        557,
                        714
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                534,
                                557,
                                714
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        534,
                                        557,
                                        714
                                    ],
                                    "score": 1.0,
                                    "content": "。RLHF有效减少了模型不符合用户期望的回答，使回复更加礼貌、有用、安全。除了 RLHF，也有研究尝试纯强化学习来训练模型。例如DeepSeek团队尝试不使用任何监督数据，仅靠 模型自我玩模拟环境（如解题尝试）获取奖励，训练出了DeepSeek\\-R1\\-Zero 等模型 。虽 然纯RL训练的模型在可读性等方面尚有不足 ，但通过配合少量有监督微调作为“冷启动”， 再加上群体策略优化（GRPO）算法，DeepSeek\\-R1 成功实现了主要依赖RL来增强推理能力的训练 范式 。这种探索表明，强化学习有望成为继自监督之后另一条提升模型智能的“轴 线” 。总结来说，大模型训练通常是“自监督预训练+有监督/指令微调+对齐调优”的多阶段 流程，各环节各司其职：预训练赋予模型广博的知识，微调塑造模型执行特定任务的技能，强化学 习则在人类偏好维度上优化模型行为，使其更符合实际应用需求。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                }
            ],
            "layout_bboxes": [],
            "page_idx": 11,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        32,
                        26,
                        558,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                26,
                                558,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        26,
                                        558,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "模型设计思路：除了开源/闭源之分，不同模型在设计上也各有侧重。有的模型追求更大的规模和多 样的技能，例如谷歌的Palm和后续的Gemini据称集成了强化学习、视觉\\-语言融合等多种技术，力 图打造“通才”模型。也有模型专注于推理能力的提升，例如OpenAI的o系列模型（o1、o3）强调增 强模型的逻辑思考过程，采用了模拟推理(Simulated Reasoning)等新技术",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_11",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        116,
                        556,
                        276
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                116,
                                556,
                                276
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        116,
                                        556,
                                        276
                                    ],
                                    "score": 1.0,
                                    "content": "。通过在生成答案前让模型进行内部的连贯“思考”，o系列模型在复杂问题上的正确率显 著提高 。类似地，Google的Gemini 2.0被报道引入了“Flash Thinking”机制，与模拟推 理异曲同工，旨在让模型更好地规划解题步骤 。还有一些模型在架构上引入模块化或稀疏 激活思想，例如DeepMind的Gopher曾探索不同模块协作，NVIDIA的Megatron-Turing提及专家 混合（MoE）的大规模应用。这类模型通过让不同部分专精不同任务，提升整体能力。而开源社区 中出现的如MPT等模型则注重长上下文，有针对性地将上下文窗口扩展到数十万token，以满足长 文档处理需求。总之，不同设计思路体现在：有的扩大模型规模追求广度，有的改良内部机制增强 深度推理，有的面向特定应用（如超长文本、多模态），体现了架构设计的多样化。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_11",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        296,
                        553,
                        520
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                296,
                                553,
                                520
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        296,
                                        553,
                                        520
                                    ],
                                    "score": 1.0,
                                    "content": "模型训练方式：大语言模型主要采用自监督学习进行预训练，具体可以是自回归或填空式的训练目 标（前文已介绍）。自回归预训练让模型学会语言生成（如GPT系列从海量文本中学习“下一词概 率”）；遮蔽式预训练让模型学会双向理解（如BERT通过在句中预测被遮挡词来学习上下文表示）。 无论哪种预训练方式，本质都是利用海量未标注文本，让模型学习语言模式和知识。这属于自监督 预训练范式，使模型具备通用语言能力。预训练完成后，还会进行监督微调和对齐调优。监督微调 使用人工标注的数据，例如将模型在翻译语料、问答对话等数据上训练，使其输出符合任务需求。 近年来更强调对模型进行指令调优，即使用人类编写的大量问答对话、指令执行示例来微调模型， 使其更善于遵循指示和进行对话。这一步骤让模型从“预测下一个词”转变为“完成用户指定的任务”。 在此基础上，领先的对话模型还采用RLHF（人类反馈强化学习）进一步打磨。具体做法是：先由 模型生成多个回答，人工或辅助模型对回答质量打分，然后通过强化学习（如PPO算法）调整模型 参数，使其倾向于得分高的回答",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_11",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        534,
                        557,
                        714
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                534,
                                557,
                                714
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        534,
                                        557,
                                        714
                                    ],
                                    "score": 1.0,
                                    "content": "。RLHF有效减少了模型不符合用户期望的回答，使回复更加礼貌、有用、安全。除了 RLHF，也有研究尝试纯强化学习来训练模型。例如DeepSeek团队尝试不使用任何监督数据，仅靠 模型自我玩模拟环境（如解题尝试）获取奖励，训练出了DeepSeek\\-R1\\-Zero 等模型 。虽 然纯RL训练的模型在可读性等方面尚有不足 ，但通过配合少量有监督微调作为“冷启动”， 再加上群体策略优化（GRPO）算法，DeepSeek\\-R1 成功实现了主要依赖RL来增强推理能力的训练 范式 。这种探索表明，强化学习有望成为继自监督之后另一条提升模型智能的“轴 线” 。总结来说，大模型训练通常是“自监督预训练+有监督/指令微调+对齐调优”的多阶段 流程，各环节各司其职：预训练赋予模型广博的知识，微调塑造模型执行特定任务的技能，强化学 习则在人类偏好维度上优化模型行为，使其更符合实际应用需求。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_11",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        32,
                        26,
                        556,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                26,
                                556,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        26,
                                        556,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "优化方法与参数高效微调：由于LLM参数极其庞大，训练和微调的计算成本惊人，因此诞生了许多 优化技术来提高效率或降低资源占用。其中一类是参数高效微调（PEFT）方法，代表之一就是前面概 念提到的LoRA（低秩适应）。LoRA方法在微调时冻结原有的大部分模型权重，仅在每层引入几个小 矩阵作为可训练参数，训练这些小矩阵来适应新任务",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        116,
                        555,
                        359
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                116,
                                555,
                                359
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        116,
                                        555,
                                        359
                                    ],
                                    "score": 1.0,
                                    "content": "DATABRICKS.COM。这些小矩阵的秩很低，因此参数量和所需算力远小于全量微调。LoRA的好处是可以 用很少的计算资源，在保持预训练模型知识的同时快速微调出新能力。 DATABRICKS.COM。许多开源模型 （如Llama、GPT\\-J等）都支持LoRA微调，大大降低了社区复用大模型的门槛。另一类重要技术是 混合专家(MoE)。这是提高训练和推理效率的架构创新，将模型的某些层扩展为包含多个“专家”子 网络的形式。 IBM.COM。每次仅激活与输入最相关的少数专家，而非让所有参数都参与计算。 IBM.COM。 这样一来，模型总参数量可以非常大（每增加一个专家就多一份参数），但实际每步计算只用到一小 部分专家，等于稀疏化了计算图。 IBM.COM。MoE使得“训练一个1万亿参数模型”的成本比等规模稠密 模型低很多，因为计算开销按激活的专家数量而非总专家数来算。谷歌的Switch Transformer和 Sparse Mixture of Expert就是MoE成功应用的例子。一些开源模型（如DeepSeek\\-V3）也采用了 MoE架构作为底座，以极大的参数规模提升预训练效果，然后通过RL等手段增强推理能力。 除了微调和架构，本身优化算法的改进也不可忽视。为训练稳定，大模型常使用自适应优化器 （如AdamW）配合分段学习率调度策略，并辅以梯度裁剪、预热等技巧防止梯度爆炸或消失。 INFOQ.COM",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        367,
                        557,
                        444
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                367,
                                557,
                                444
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        367,
                                        557,
                                        444
                                    ],
                                    "score": 1.0,
                                    "content": "Facebook研究发现规模化法律（Scaling Laws），指导在给定算力下如何平衡参数规模与训练步骤 以最优地提升模型性能，这也是训练策略优化的一部分。总之，从LoRA这类微调技巧，到MoE这样 的架构调整，再到训练过程超参数的精细调控，这些优化方法共同支撑了如今LLM的高效训练和部 署，让我们能在相对可控的资源下驯服超大规模的模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                }
            ],
            "layout_bboxes": [],
            "page_idx": 12,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        32,
                        26,
                        556,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                26,
                                556,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        26,
                                        556,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "优化方法与参数高效微调：由于LLM参数极其庞大，训练和微调的计算成本惊人，因此诞生了许多 优化技术来提高效率或降低资源占用。其中一类是参数高效微调（PEFT）方法，代表之一就是前面概 念提到的LoRA（低秩适应）。LoRA方法在微调时冻结原有的大部分模型权重，仅在每层引入几个小 矩阵作为可训练参数，训练这些小矩阵来适应新任务",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_12",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        116,
                        555,
                        359
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                116,
                                555,
                                359
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        116,
                                        555,
                                        359
                                    ],
                                    "score": 1.0,
                                    "content": "DATABRICKS.COM。这些小矩阵的秩很低，因此参数量和所需算力远小于全量微调。LoRA的好处是可以 用很少的计算资源，在保持预训练模型知识的同时快速微调出新能力。 DATABRICKS.COM。许多开源模型 （如Llama、GPT\\-J等）都支持LoRA微调，大大降低了社区复用大模型的门槛。另一类重要技术是 混合专家(MoE)。这是提高训练和推理效率的架构创新，将模型的某些层扩展为包含多个“专家”子 网络的形式。 IBM.COM。每次仅激活与输入最相关的少数专家，而非让所有参数都参与计算。 IBM.COM。 这样一来，模型总参数量可以非常大（每增加一个专家就多一份参数），但实际每步计算只用到一小 部分专家，等于稀疏化了计算图。 IBM.COM。MoE使得“训练一个1万亿参数模型”的成本比等规模稠密 模型低很多，因为计算开销按激活的专家数量而非总专家数来算。谷歌的Switch Transformer和 Sparse Mixture of Expert就是MoE成功应用的例子。一些开源模型（如DeepSeek\\-V3）也采用了 MoE架构作为底座，以极大的参数规模提升预训练效果，然后通过RL等手段增强推理能力。 除了微调和架构，本身优化算法的改进也不可忽视。为训练稳定，大模型常使用自适应优化器 （如AdamW）配合分段学习率调度策略，并辅以梯度裁剪、预热等技巧防止梯度爆炸或消失。 INFOQ.COM",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_12",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        367,
                        557,
                        444
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                367,
                                557,
                                444
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        367,
                                        557,
                                        444
                                    ],
                                    "score": 1.0,
                                    "content": "Facebook研究发现规模化法律（Scaling Laws），指导在给定算力下如何平衡参数规模与训练步骤 以最优地提升模型性能，这也是训练策略优化的一部分。总之，从LoRA这类微调技巧，到MoE这样 的架构调整，再到训练过程超参数的精细调控，这些优化方法共同支撑了如今LLM的高效训练和部 署，让我们能在相对可控的资源下驯服超大规模的模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_12",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        33,
                        26,
                        554,
                        144
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                26,
                                554,
                                144
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        26,
                                        554,
                                        144
                                    ],
                                    "score": 1.0,
                                    "content": "推理加速技术：大模型在推理阶段的加速同样至关重要，直接影响实际应用的速度和成本。常用的 加速手段之一是模型压缩。除了上述量化和蒸馏，另一种是权重剪枝(Pruning)，即删除对最终结 果贡献很小的连接或神经元，从而减小模型规模。剪枝可以在几乎不损失精度的情况下大幅压缩模 型，比如一些研究对GPT类模型剪枝30%参数而性能基本不变。图优化也是重要方向，即通过底层 实现改进提高计算效率。例如自注意力计算可用更高效的算法（如FlashAttention）实现，以减少 显存访问和计算冗余，从而加速数倍。又如针对Transformer结构的深度，提出的重计算",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        152,
                        556,
                        439
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                152,
                                555,
                                439
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        152,
                                        555,
                                        439
                                    ],
                                    "score": 1.0,
                                    "content": "(Recompute)技术在不增加显存前提下通过多次计算换时间，平衡了存储和算力。硬件方面，混合 精度推理已经成为标准做法：使用FP16甚至更低精度进行矩阵乘法运算，同时保留少部分关键运算 用高精度，以兼顾速度和稳定性。现代GPU和TPU都对低精度计算做了优化，使模型在推理时能跑 得更快。批处理推理可以提升吞吐——将多个输入打包一起推理，充分利用矩阵运算的并行性，但 这需要有足够请求量支撑。对于需要低延迟的交互式应用，还可以采用多路复用技术，让一个大模 型实例同时服务多个会话而不明显减速。缓存机制也是实用的加速方案，例如对Transformer解码 过程的中间状态缓存，避免每生成一个词就重复计算先前的注意力。这在长文本生成时效果显著。 最后，随着模型规模增长，专用加速硬件的作用愈发突出。除了GPU，Google TPU、亚马逊 Inferentia、寒武纪MLU等AI芯片都针对Transformer推理进行了优化。比如一些芯片有稀疏计算加 速单元，专门加速MoE模型的专家路由计算。再如大显存硬件允许将模型完全放入内存，避免频繁 的数据交换延迟。综合运用这些技术，当前已经可以在消费级硬件上以量化4-bit形式跑一个数十亿 参数的模型，在云端通过高度优化的集群实时提供千亿参数模型的响应。这些推理加速手段保证了 大模型能够以可用的速度和成本服务于实际应用，从手机上的离线文本生成，到服务器上的海量请 求处理。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        475,
                        333,
                        492
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                475,
                                333,
                                492
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        475,
                                        333,
                                        492
                                    ],
                                    "score": 1.0,
                                    "content": "2024 年及以后发布的主流大模型介绍",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        519,
                        543,
                        552
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                519,
                                543,
                                552
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        519,
                                        543,
                                        552
                                    ],
                                    "score": 1.0,
                                    "content": "近年涌现了许多新的大语言模型，下面介绍几款在2024年及以后发布或知名的模型，它们各有特 色、应用场景和技术创新：",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        573,
                        488,
                        585
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                573,
                                488,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        573,
                                        488,
                                        585
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet GPT-4o：OpenAI于2024年5月发布的旗舰模型，“4o”中的“o”代表“omni”（全能）",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        40,
                        588,
                        558,
                        774
                    ],
                    "lines": [
                        {
                            "bbox": [
                                40,
                                588,
                                558,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        40,
                                        588,
                                        558,
                                        774
                                    ],
                                    "score": 1.0,
                                    "content": "EN.WIKIPEDIA.ORG 。GPT\\-4o是多模态多语言模型，能处理文本、图像和音频输入，并实时地进行响 应 EN.WIKIPEDIA.ORG 。它在语音识别和翻译等领域创下新纪录，英文以外语言和视觉任务上比GPT\\-4 Turbo有明显优势 EN.WIKIPEDIA.ORG 。GPT\\-4o支持50多种语言输入输出，并提供了原生的语音对语 音对话功能 EN.WIKIPEDIA.ORG 。面向公众，GPT\\-4o成为ChatGPT的核心模型（其中GPT\\-4o基础版免 费向所有用户开放，但高级用户有更高的频率限制 EN.WIKIPEDIA.ORG ）。技术上，GPT\\-4o在架构上延 续了GPT\\-4的decoder\\-onlyTransformer，但经过优化训练实现了更快的API响应和更低的使用 成本 EN.WIKIPEDIA.ORG 。它在MMLU等Benchmark上略胜此前的GPT\\-4，在多语言、多模态对话方 面树立了行业新标杆 EN.WIKIPEDIA.ORG 。主要应用场景包括对话问答、多语言客服、语音助手和内 容创作等。作为GPT\\-4的增强版，GPT\\-4o体现了OpenAI在大模型多能化方向的探索成果。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                }
            ],
            "layout_bboxes": [],
            "page_idx": 13,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        33,
                        26,
                        554,
                        144
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                26,
                                554,
                                144
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        26,
                                        554,
                                        144
                                    ],
                                    "score": 1.0,
                                    "content": "推理加速技术：大模型在推理阶段的加速同样至关重要，直接影响实际应用的速度和成本。常用的 加速手段之一是模型压缩。除了上述量化和蒸馏，另一种是权重剪枝(Pruning)，即删除对最终结 果贡献很小的连接或神经元，从而减小模型规模。剪枝可以在几乎不损失精度的情况下大幅压缩模 型，比如一些研究对GPT类模型剪枝30%参数而性能基本不变。图优化也是重要方向，即通过底层 实现改进提高计算效率。例如自注意力计算可用更高效的算法（如FlashAttention）实现，以减少 显存访问和计算冗余，从而加速数倍。又如针对Transformer结构的深度，提出的重计算",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_13",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        152,
                        556,
                        439
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                152,
                                555,
                                439
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        152,
                                        555,
                                        439
                                    ],
                                    "score": 1.0,
                                    "content": "(Recompute)技术在不增加显存前提下通过多次计算换时间，平衡了存储和算力。硬件方面，混合 精度推理已经成为标准做法：使用FP16甚至更低精度进行矩阵乘法运算，同时保留少部分关键运算 用高精度，以兼顾速度和稳定性。现代GPU和TPU都对低精度计算做了优化，使模型在推理时能跑 得更快。批处理推理可以提升吞吐——将多个输入打包一起推理，充分利用矩阵运算的并行性，但 这需要有足够请求量支撑。对于需要低延迟的交互式应用，还可以采用多路复用技术，让一个大模 型实例同时服务多个会话而不明显减速。缓存机制也是实用的加速方案，例如对Transformer解码 过程的中间状态缓存，避免每生成一个词就重复计算先前的注意力。这在长文本生成时效果显著。 最后，随着模型规模增长，专用加速硬件的作用愈发突出。除了GPU，Google TPU、亚马逊 Inferentia、寒武纪MLU等AI芯片都针对Transformer推理进行了优化。比如一些芯片有稀疏计算加 速单元，专门加速MoE模型的专家路由计算。再如大显存硬件允许将模型完全放入内存，避免频繁 的数据交换延迟。综合运用这些技术，当前已经可以在消费级硬件上以量化4-bit形式跑一个数十亿 参数的模型，在云端通过高度优化的集群实时提供千亿参数模型的响应。这些推理加速手段保证了 大模型能够以可用的速度和成本服务于实际应用，从手机上的离线文本生成，到服务器上的海量请 求处理。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_13",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "title",
                    "bbox": [
                        33,
                        475,
                        333,
                        492
                    ],
                    "lines": [
                        {
                            "bbox": [
                                33,
                                475,
                                333,
                                492
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        33,
                                        475,
                                        333,
                                        492
                                    ],
                                    "score": 1.0,
                                    "content": "2024 年及以后发布的主流大模型介绍",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_13",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        519,
                        543,
                        552
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                519,
                                543,
                                552
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        519,
                                        543,
                                        552
                                    ],
                                    "score": 1.0,
                                    "content": "近年涌现了许多新的大语言模型，下面介绍几款在2024年及以后发布或知名的模型，它们各有特 色、应用场景和技术创新：",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_13",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        38,
                        573,
                        488,
                        585
                    ],
                    "lines": [
                        {
                            "bbox": [
                                38,
                                573,
                                488,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        38,
                                        573,
                                        488,
                                        585
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet GPT-4o：OpenAI于2024年5月发布的旗舰模型，“4o”中的“o”代表“omni”（全能）",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_13",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        40,
                        588,
                        558,
                        774
                    ],
                    "lines": [
                        {
                            "bbox": [
                                40,
                                588,
                                558,
                                774
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        40,
                                        588,
                                        558,
                                        774
                                    ],
                                    "score": 1.0,
                                    "content": "EN.WIKIPEDIA.ORG 。GPT\\-4o是多模态多语言模型，能处理文本、图像和音频输入，并实时地进行响 应 EN.WIKIPEDIA.ORG 。它在语音识别和翻译等领域创下新纪录，英文以外语言和视觉任务上比GPT\\-4 Turbo有明显优势 EN.WIKIPEDIA.ORG 。GPT\\-4o支持50多种语言输入输出，并提供了原生的语音对语 音对话功能 EN.WIKIPEDIA.ORG 。面向公众，GPT\\-4o成为ChatGPT的核心模型（其中GPT\\-4o基础版免 费向所有用户开放，但高级用户有更高的频率限制 EN.WIKIPEDIA.ORG ）。技术上，GPT\\-4o在架构上延 续了GPT\\-4的decoder\\-onlyTransformer，但经过优化训练实现了更快的API响应和更低的使用 成本 EN.WIKIPEDIA.ORG 。它在MMLU等Benchmark上略胜此前的GPT\\-4，在多语言、多模态对话方 面树立了行业新标杆 EN.WIKIPEDIA.ORG 。主要应用场景包括对话问答、多语言客服、语音助手和内 容创作等。作为GPT\\-4的增强版，GPT\\-4o体现了OpenAI在大模型多能化方向的探索成果。",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_13",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        55,
                        21,
                        555,
                        96
                    ],
                    "lines": [
                        {
                            "bbox": [
                                55,
                                21,
                                555,
                                96
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        55,
                                        21,
                                        555,
                                        96
                                    ],
                                    "score": 1.0,
                                    "content": "OpenAI O1：这是OpenAI在GPT\\-4o之后推出的新一代大模型系列（内部代号O系列）的首批 模型。O1可以看作GPT\\-4.5级别的通用模型，其参数规模和训练数据在GPT\\-4o基础上进一步提 升，被用于ChatGPT等产品的新版本中。O1模型强调可靠性和广泛任务性能，是后续O3发展 出的“推理加强”版本的基础",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        54,
                        105,
                        556,
                        265
                    ],
                    "lines": [
                        {
                            "bbox": [
                                54,
                                107,
                                556,
                                265
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        54,
                                        107,
                                        556,
                                        265
                                    ],
                                    "score": 1.0,
                                    "content": "。在Benchmark上，OpenAI的o1与同时期其他领先模型（如Claude 3.5、 DeepSeek-R1等）不相上下 。它擅长语言理解、推理和代码生成等各类任务，是 OpenAI在2024年下半年对外提供API的主要模型之一。技术创新方面，O1继续优化了多语种 和多模态能力，相比GPT\\-4o在推理链质量和响应一致性上有所改进，减少了胡乱编造和不一致 的情况。据社区反馈，O1的对话风格更自然，遵从指令的稳定性更高，这可能得益于更完善的 RLHF调教和更大、更干净的数据集。作为闭源模型，O1具体架构细节未公布，但它为后来更 先进的O3系列奠定了基础，是2024年主流闭源大模型之一。应用上，O1被用于需要高可靠性的 商业场景，如企业知识问答、编程助手等。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        278,
                        558,
                        312
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                278,
                                558,
                                312
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        278,
                                        558,
                                        312
                                    ],
                                    "score": 1.0,
                                    "content": "OpenAI O3-mini：O3是OpenAI在2024年底公布的新模型系列，主打推理能力提升。其中o3- mini于2025年初率先发布，是o3的轻量版本",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        55,
                        321,
                        559,
                        375
                    ],
                    "lines": [
                        {
                            "bbox": [
                                55,
                                321,
                                559,
                                375
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        55,
                                        321,
                                        559,
                                        375
                                    ],
                                    "score": 1.0,
                                    "content": "HELICONE.AI 。O3-mini定位为更快更廉价但仍具强大推理能力的模型，相比上一代的o1-mini，错 误率减少了39%，响应速度提高了24% HELICONE.AI 。它提供低、中、高三个“思考强度”模式，用 户可权衡速度与准确性要求 HELICONE.AI 。技术上，o3系列引入了前文提及的**模拟推理",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        51,
                        383,
                        560,
                        585
                    ],
                    "lines": [
                        {
                            "bbox": [
                                51,
                                383,
                                560,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        51,
                                        383,
                                        560,
                                        585
                                    ],
                                    "score": 1.0,
                                    "content": "(Simulated Reasoning)**机制，使模型在回答前可以进行内部的“思考和规划” 。具体 来说，o3会先隐式地产生一段链式推理过程，再据此组织答案，这使其在复杂数学、科学问答 上表现远胜以往模型 HELICON.AI 。o3\\-mini尽管参数比完整版o3少，但依然继承了这种推理增强 能力，因而在许多推理密集型Benchmark上超越了规模更大的上一代模型。例如在AIME 2024 数学竞赛题上，o3\\-mini略胜DeepSeek\\-R1 HELICON.AI ；不过在GPQA钻石难题上，o3\\-mini仍稍 逊于DeepSeek\\-R1，显示出进一步提升空间 HELICON.AI 。o3\\-mini的另一个亮点是成本显著降 低，据OpenAI称其使用费用仅为同级别o1模型的37% HELICON.AI 。这使开发者可以用更低开销 获得接近顶尖的性能。主要应用方面，o3\\-mini适合需要较高推理准确又受限于延迟和成本的场 景，例如实时客服机器人、教育辅导系统等。它也通过ChatGPT界面和OpenAI API提供服务， 是2025年初备受关注的一款高性价比大模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        54,
                        600,
                        558,
                        695
                    ],
                    "lines": [
                        {
                            "bbox": [
                                54,
                                600,
                                558,
                                695
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        54,
                                        600,
                                        558,
                                        695
                                    ],
                                    "score": 1.0,
                                    "content": "Gemini 2.0 Pro：Google在继PaLM 2和初代Gemini之后的新一代通用大模型。据报道， Gemini 2.0 在2024年底亮相，其高配版本称为Pro，定位为全面对标并超越GPT\\-4系列的旗舰 模型。Gemini 2.0 Pro 是多模态模型，融合了语言、视觉和工具调用能力。它采用了Google深 厚的搜索和知识图谱积累，擅长实时信息获取和知识问答。据Helicone透露，Gemini 2.0引入 了类似OpenAI模拟推理的“Flash Thinking”技术，以增强模型的逐步推理和计划能力",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "text",
                    "bbox": [
                        51,
                        700,
                        553,
                        780
                    ],
                    "lines": [
                        {
                            "bbox": [
                                51,
                                700,
                                553,
                                780
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        51,
                                        700,
                                        553,
                                        780
                                    ],
                                    "score": 1.0,
                                    "content": "HELICON.AI 。这意味着Gemini 2.0 Pro在复杂推理题上也会进行内部多步计算，从而提升准确 率。技术创新上，Gemini 2.0 Pro据传在架构上结合了Transformer和强化学习策略，并利 用大规模多语言、多模态数据进行训练，使其具有跨领域的问题解决能力和复杂任务分解能 力。例如，它可以读懂一张科学图表并用语言解释含义，或者根据用户模糊的高层指令自动拆",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6
                }
            ],
            "layout_bboxes": [],
            "page_idx": 14,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        55,
                        21,
                        555,
                        96
                    ],
                    "lines": [
                        {
                            "bbox": [
                                55,
                                21,
                                555,
                                96
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        55,
                                        21,
                                        555,
                                        96
                                    ],
                                    "score": 1.0,
                                    "content": "OpenAI O1：这是OpenAI在GPT\\-4o之后推出的新一代大模型系列（内部代号O系列）的首批 模型。O1可以看作GPT\\-4.5级别的通用模型，其参数规模和训练数据在GPT\\-4o基础上进一步提 升，被用于ChatGPT等产品的新版本中。O1模型强调可靠性和广泛任务性能，是后续O3发展 出的“推理加强”版本的基础",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_14",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        54,
                        105,
                        556,
                        265
                    ],
                    "lines": [
                        {
                            "bbox": [
                                54,
                                107,
                                556,
                                265
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        54,
                                        107,
                                        556,
                                        265
                                    ],
                                    "score": 1.0,
                                    "content": "。在Benchmark上，OpenAI的o1与同时期其他领先模型（如Claude 3.5、 DeepSeek-R1等）不相上下 。它擅长语言理解、推理和代码生成等各类任务，是 OpenAI在2024年下半年对外提供API的主要模型之一。技术创新方面，O1继续优化了多语种 和多模态能力，相比GPT\\-4o在推理链质量和响应一致性上有所改进，减少了胡乱编造和不一致 的情况。据社区反馈，O1的对话风格更自然，遵从指令的稳定性更高，这可能得益于更完善的 RLHF调教和更大、更干净的数据集。作为闭源模型，O1具体架构细节未公布，但它为后来更 先进的O3系列奠定了基础，是2024年主流闭源大模型之一。应用上，O1被用于需要高可靠性的 商业场景，如企业知识问答、编程助手等。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_14",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        278,
                        558,
                        312
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                278,
                                558,
                                312
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        278,
                                        558,
                                        312
                                    ],
                                    "score": 1.0,
                                    "content": "OpenAI O3-mini：O3是OpenAI在2024年底公布的新模型系列，主打推理能力提升。其中o3- mini于2025年初率先发布，是o3的轻量版本",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_14",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        55,
                        321,
                        559,
                        375
                    ],
                    "lines": [
                        {
                            "bbox": [
                                55,
                                321,
                                559,
                                375
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        55,
                                        321,
                                        559,
                                        375
                                    ],
                                    "score": 1.0,
                                    "content": "HELICONE.AI 。O3-mini定位为更快更廉价但仍具强大推理能力的模型，相比上一代的o1-mini，错 误率减少了39%，响应速度提高了24% HELICONE.AI 。它提供低、中、高三个“思考强度”模式，用 户可权衡速度与准确性要求 HELICONE.AI 。技术上，o3系列引入了前文提及的**模拟推理",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_14",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        51,
                        383,
                        560,
                        585
                    ],
                    "lines": [
                        {
                            "bbox": [
                                51,
                                383,
                                560,
                                585
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        51,
                                        383,
                                        560,
                                        585
                                    ],
                                    "score": 1.0,
                                    "content": "(Simulated Reasoning)**机制，使模型在回答前可以进行内部的“思考和规划” 。具体 来说，o3会先隐式地产生一段链式推理过程，再据此组织答案，这使其在复杂数学、科学问答 上表现远胜以往模型 HELICON.AI 。o3\\-mini尽管参数比完整版o3少，但依然继承了这种推理增强 能力，因而在许多推理密集型Benchmark上超越了规模更大的上一代模型。例如在AIME 2024 数学竞赛题上，o3\\-mini略胜DeepSeek\\-R1 HELICON.AI ；不过在GPQA钻石难题上，o3\\-mini仍稍 逊于DeepSeek\\-R1，显示出进一步提升空间 HELICON.AI 。o3\\-mini的另一个亮点是成本显著降 低，据OpenAI称其使用费用仅为同级别o1模型的37% HELICON.AI 。这使开发者可以用更低开销 获得接近顶尖的性能。主要应用方面，o3\\-mini适合需要较高推理准确又受限于延迟和成本的场 景，例如实时客服机器人、教育辅导系统等。它也通过ChatGPT界面和OpenAI API提供服务， 是2025年初备受关注的一款高性价比大模型。",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_14",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        54,
                        600,
                        558,
                        695
                    ],
                    "lines": [
                        {
                            "bbox": [
                                54,
                                600,
                                558,
                                695
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        54,
                                        600,
                                        558,
                                        695
                                    ],
                                    "score": 1.0,
                                    "content": "Gemini 2.0 Pro：Google在继PaLM 2和初代Gemini之后的新一代通用大模型。据报道， Gemini 2.0 在2024年底亮相，其高配版本称为Pro，定位为全面对标并超越GPT\\-4系列的旗舰 模型。Gemini 2.0 Pro 是多模态模型，融合了语言、视觉和工具调用能力。它采用了Google深 厚的搜索和知识图谱积累，擅长实时信息获取和知识问答。据Helicone透露，Gemini 2.0引入 了类似OpenAI模拟推理的“Flash Thinking”技术，以增强模型的逐步推理和计划能力",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_14",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        51,
                        700,
                        553,
                        780
                    ],
                    "lines": [
                        {
                            "bbox": [
                                51,
                                700,
                                553,
                                780
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        51,
                                        700,
                                        553,
                                        780
                                    ],
                                    "score": 1.0,
                                    "content": "HELICON.AI 。这意味着Gemini 2.0 Pro在复杂推理题上也会进行内部多步计算，从而提升准确 率。技术创新上，Gemini 2.0 Pro据传在架构上结合了Transformer和强化学习策略，并利 用大规模多语言、多模态数据进行训练，使其具有跨领域的问题解决能力和复杂任务分解能 力。例如，它可以读懂一张科学图表并用语言解释含义，或者根据用户模糊的高层指令自动拆",
                                    "type": "text"
                                }
                            ],
                            "index": 6
                        }
                    ],
                    "index": 6,
                    "page_num": "page_14",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        56,
                        21,
                        552,
                        75
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                21,
                                552,
                                75
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        21,
                                        552,
                                        75
                                    ],
                                    "score": 1.0,
                                    "content": "解成子任务并逐一完成。凭借Google TPU v5集群的强大算力支持，Gemini 2.0的训练规模空 前，其Pro版本参数量和数据规模达到业界顶尖。主要应用场景包括Google自身的产品（如升 级版Bard聊天助手、Google Docs中的AI助手等），以及云端Vertex AI提供的模型服务。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        56,
                        84,
                        554,
                        117
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                84,
                                554,
                                117
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        84,
                                        554,
                                        117
                                    ],
                                    "score": 1.0,
                                    "content": "Gemini 2.0 Pro被视为Google冲击AI领军地位的力作，在多模态推理、多步骤问题解决上代表 了2024年底的最高水准之一。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        131,
                        559,
                        186
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                131,
                                559,
                                186
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        131,
                                        559,
                                        186
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet Claude 3.5 Sonnet：Anthropic公司2024年中推出的Claude 3.5系列首个型号，代号“Sonnet” \\boxed{ANTHROPIC.COM} 。Claude 3.5 Sonnet显著提高了智能水平，被Anthropic称为“行业智能的新标杆” \\boxed{ANTHROPIC.COM} 。相较之前的Claude 3，它在广泛评测上都有提升，包括研究生水平推理",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        55,
                        193,
                        557,
                        459
                    ],
                    "lines": [
                        {
                            "bbox": [
                                55,
                                193,
                                557,
                                459
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        55,
                                        193,
                                        557,
                                        459
                                    ],
                                    "score": 1.0,
                                    "content": "（GPQA）、本科水平知识测验（MMLU）和编程能力（HumanEval）等。Claude 3.5 Sonnet还将上下文窗口扩展到惊人的200K tokens（约相当于15万字），可以 处理超长文档而一次性输出结果，非常适合长篇文章分析、长对话记忆等需求。性能提升的同 时，它的响应速度是上一代Claude 3 Opus的两倍。同时，它的响应速度是上一代Claude 3 Opus的两倍。 技术创新方面，Sonnet在模型 架构和训练上作了多处优化：引入了结构化思维和更好的对话语气，善于理解细微语义差别、 幽默等高级特征；在代码能力上，通过内部构建的“代理式”编码评测，它能够根据 自然语言描述自主编写、调试和执行代码；在多模态上，Sonnet成为Anthropic目 前最强的视觉模型，不仅能看图识别，还能解读复杂图表，甚至从模糊照片中准确提取文字 。Claude 3.5 Sonnet提供免费网页版体验，并可通过API调用，定价与中等规模模 型相当，但提供了接近顶尖模型的能力。其主要应用场景涵盖代码助手（集成进开 发者工具链）、客服和办公助理（利用长上下文处理整本手册或长对话）、以及需要视觉和文本 综合分析的领域（如物流、金融票据处理等）。Claude 3.5 Sonnet以高效率、高上 下文和多模态著称，是2024年中大模型领域的一大亮点。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "text",
                    "bbox": [
                        40,
                        474,
                        555,
                        507
                    ],
                    "lines": [
                        {
                            "bbox": [
                                40,
                                474,
                                555,
                                507
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        40,
                                        474,
                                        555,
                                        507
                                    ],
                                    "score": 1.0,
                                    "content": "Llama 3：Meta公司延续开源路线，于2024年推出了Llama系列的第三版。Llama 3在2024年 $\\bullet$ 4月首先发布8B和70B两个规模的模型",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "text",
                    "bbox": [
                        53,
                        516,
                        559,
                        759
                    ],
                    "lines": [
                        {
                            "bbox": [
                                53,
                                516,
                                559,
                                759
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        53,
                                        516,
                                        559,
                                        759
                                    ],
                                    "score": 1.0,
                                    "content": "KAVOUT.COM 。与前代相比，Llama 3在训练数据和架构上都做了改进，支持更多语言和编码，适 应范围更广。7月，Meta发布Llama 3.1，其中包括一个高达4050亿参数的变体 ——这被认为是迄今最大规模的开源模型之一，展示了Meta在开源领域的野心 Llama 3系列模型在开源社区中快速传播，其权重在遵循社区许可证的前提下开放获取 。模型在中文、英语等多语言上表现出色，并具备一定的代码和逻辑推理能力。 Meta还将Llama 3集成到了自己的Meta AI助手中，通过Facebook、Instagram、WhatsApp 等应用为用户提供对话服务 。这使得Llama 3直接服务数亿用户，成为真正大规模 落地的开源模型。技术创新方面，Llama 3在预训练阶段引入了更多高质量数据，尤其是代码和 对话数据，提高了模型在编码和聊天上的能力；另外在训练效率上使用了一系列优化（如 FlashAttention、高效并行等），使得405B模型的训练成为可能。Llama 3延续了开源可商用的 许可证策略，开发者可以在其上微调开发自有应用，这也催生了大量Llama 3的变体和细化版本 在社区涌现（如Llama\\-3\\-Chinese、Llama\\-3\\-长上下文版等）。主要应用场景包括各类聊天机器",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5
                }
            ],
            "layout_bboxes": [],
            "page_idx": 15,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        56,
                        21,
                        552,
                        75
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                21,
                                552,
                                75
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        21,
                                        552,
                                        75
                                    ],
                                    "score": 1.0,
                                    "content": "解成子任务并逐一完成。凭借Google TPU v5集群的强大算力支持，Gemini 2.0的训练规模空 前，其Pro版本参数量和数据规模达到业界顶尖。主要应用场景包括Google自身的产品（如升 级版Bard聊天助手、Google Docs中的AI助手等），以及云端Vertex AI提供的模型服务。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_15",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        56,
                        84,
                        554,
                        117
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                84,
                                554,
                                117
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        84,
                                        554,
                                        117
                                    ],
                                    "score": 1.0,
                                    "content": "Gemini 2.0 Pro被视为Google冲击AI领军地位的力作，在多模态推理、多步骤问题解决上代表 了2024年底的最高水准之一。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_15",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        39,
                        131,
                        559,
                        186
                    ],
                    "lines": [
                        {
                            "bbox": [
                                39,
                                131,
                                559,
                                186
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        39,
                                        131,
                                        559,
                                        186
                                    ],
                                    "score": 1.0,
                                    "content": "\\bullet Claude 3.5 Sonnet：Anthropic公司2024年中推出的Claude 3.5系列首个型号，代号“Sonnet” \\boxed{ANTHROPIC.COM} 。Claude 3.5 Sonnet显著提高了智能水平，被Anthropic称为“行业智能的新标杆” \\boxed{ANTHROPIC.COM} 。相较之前的Claude 3，它在广泛评测上都有提升，包括研究生水平推理",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_15",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        55,
                        193,
                        557,
                        459
                    ],
                    "lines": [
                        {
                            "bbox": [
                                55,
                                193,
                                557,
                                459
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        55,
                                        193,
                                        557,
                                        459
                                    ],
                                    "score": 1.0,
                                    "content": "（GPQA）、本科水平知识测验（MMLU）和编程能力（HumanEval）等。Claude 3.5 Sonnet还将上下文窗口扩展到惊人的200K tokens（约相当于15万字），可以 处理超长文档而一次性输出结果，非常适合长篇文章分析、长对话记忆等需求。性能提升的同 时，它的响应速度是上一代Claude 3 Opus的两倍。同时，它的响应速度是上一代Claude 3 Opus的两倍。 技术创新方面，Sonnet在模型 架构和训练上作了多处优化：引入了结构化思维和更好的对话语气，善于理解细微语义差别、 幽默等高级特征；在代码能力上，通过内部构建的“代理式”编码评测，它能够根据 自然语言描述自主编写、调试和执行代码；在多模态上，Sonnet成为Anthropic目 前最强的视觉模型，不仅能看图识别，还能解读复杂图表，甚至从模糊照片中准确提取文字 。Claude 3.5 Sonnet提供免费网页版体验，并可通过API调用，定价与中等规模模 型相当，但提供了接近顶尖模型的能力。其主要应用场景涵盖代码助手（集成进开 发者工具链）、客服和办公助理（利用长上下文处理整本手册或长对话）、以及需要视觉和文本 综合分析的领域（如物流、金融票据处理等）。Claude 3.5 Sonnet以高效率、高上 下文和多模态著称，是2024年中大模型领域的一大亮点。",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_15",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        40,
                        474,
                        555,
                        507
                    ],
                    "lines": [
                        {
                            "bbox": [
                                40,
                                474,
                                555,
                                507
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        40,
                                        474,
                                        555,
                                        507
                                    ],
                                    "score": 1.0,
                                    "content": "Llama 3：Meta公司延续开源路线，于2024年推出了Llama系列的第三版。Llama 3在2024年 $\\bullet$ 4月首先发布8B和70B两个规模的模型",
                                    "type": "text"
                                }
                            ],
                            "index": 4
                        }
                    ],
                    "index": 4,
                    "page_num": "page_15",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        53,
                        516,
                        559,
                        759
                    ],
                    "lines": [
                        {
                            "bbox": [
                                53,
                                516,
                                559,
                                759
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        53,
                                        516,
                                        559,
                                        759
                                    ],
                                    "score": 1.0,
                                    "content": "KAVOUT.COM 。与前代相比，Llama 3在训练数据和架构上都做了改进，支持更多语言和编码，适 应范围更广。7月，Meta发布Llama 3.1，其中包括一个高达4050亿参数的变体 ——这被认为是迄今最大规模的开源模型之一，展示了Meta在开源领域的野心 Llama 3系列模型在开源社区中快速传播，其权重在遵循社区许可证的前提下开放获取 。模型在中文、英语等多语言上表现出色，并具备一定的代码和逻辑推理能力。 Meta还将Llama 3集成到了自己的Meta AI助手中，通过Facebook、Instagram、WhatsApp 等应用为用户提供对话服务 。这使得Llama 3直接服务数亿用户，成为真正大规模 落地的开源模型。技术创新方面，Llama 3在预训练阶段引入了更多高质量数据，尤其是代码和 对话数据，提高了模型在编码和聊天上的能力；另外在训练效率上使用了一系列优化（如 FlashAttention、高效并行等），使得405B模型的训练成为可能。Llama 3延续了开源可商用的 许可证策略，开发者可以在其上微调开发自有应用，这也催生了大量Llama 3的变体和细化版本 在社区涌现（如Llama\\-3\\-Chinese、Llama\\-3\\-长上下文版等）。主要应用场景包括各类聊天机器",
                                    "type": "text"
                                }
                            ],
                            "index": 5
                        }
                    ],
                    "index": 5,
                    "page_num": "page_15",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        56,
                        21,
                        560,
                        54
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                21,
                                560,
                                54
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        21,
                                        560,
                                        54
                                    ],
                                    "score": 1.0,
                                    "content": "人、文本生成工具、多语言翻译和垂直领域知识问答等。作为开源领域的领头羊，Llama 3证明了开放模型也能达到顶尖性能，并通过免费提供使用打破了专有模型的垄断。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        56,
                        69,
                        558,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                69,
                                558,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        69,
                                        558,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "DeepSeek R1：由中国初创公司DeepSeek在2024年底发布的开源大型模型。DeepSeek-R1引 人注目之处在于其卓越的推理能力和开放可获得。据技术报告，R1在许多Benchmark上达到或",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        42,
                        123,
                        558,
                        439
                    ],
                    "lines": [
                        {
                            "bbox": [
                                42,
                                123,
                                558,
                                439
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        42,
                                        123,
                                        558,
                                        439
                                    ],
                                    "score": 1.0,
                                    "content": "。尤其在数学和编程任务上，DeepSeek-R1表现突出，被评价为“开放源代码领域对闭 源领先模型的有力挑战” 。DeepSeek-V3 底座模型，这是一个Mixture-of-Experts架构的大模型，使基础模型具有超大参 数容量 。在此之上，R1采用了独特的训练流程：首先进行纯强化学习预训练（产生 DeepSeek-R1-Zero模型） ，然后辅以少量有监督SFT稳定语言输出，最后再次通过强 化学习（GRPO算法）强化推理能力 。这种重RL轻监督的训练策略使R1具备 了非凡的链式推理和自主思考能力。技术报告指出，DeepSeek-R1在无需大量人工标注数据的 情况下，单靠自我进化就获得了强大的推理本领 。在长上下文处理方面，R1也经过优 化，可以在较长文本和复杂问题上保持上下文一致。DeepSeek-R1开源了模型权重和代码，实 现了在商用GPU上的部署优化 。社区很快提供了R1的多种蒸馏小模型，甚至通过知识 蒸馏到驯化的Llama和Qwen模型上，使得这些小模型在数学和编码上超越了原本更大的模型 。DeepSeek-R1的主要应用场景包括学术研究（因为其推理能力强，适合科学问 答）、高级编程助手（在代码挑战上表现优异）、以及需要严谨逐步推理的业务场景（如法律分 析助手等）。作为一个开源项目，R1还受到社区安全测试的挑战并不断完善，是中国在大模型 核心技术上取得突破的代表之一。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "text",
                    "bbox": [
                        42,
                        454,
                        558,
                        781
                    ],
                    "lines": [
                        {
                            "bbox": [
                                42,
                                454,
                                558,
                                781
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        42,
                                        454,
                                        558,
                                        781
                                    ],
                                    "score": 1.0,
                                    "content": "$\\bullet$ Kimi k1.5：Moonshot AI公司（成立于2023年的中国AI新创）推出的最新LLM版本 。Kimi k1.5是一款多模态开源模型，免费向公众提供使用，并提供网页版聊天 接口 。ANALYTCSVIDHYA.COM 。ANALYTCSVIDHYA.COM 。它有几个显著特点：首先是超长上下文——Kimi k1.5支 持128K tokens的上下文窗口 。ANALYTCSVIDHYA.COM ，能一次处理海量文本或多个文件输入。其次， Kimi k1.5内置了实时网页搜索和多文件分析能力，可在聊天中直接检索信息或解析PDF/图片等 文件 。例如，用户可以让它同时分析多达50个文档（PDF、Word、图片等）并 综合给出结果 。ANALYTCSVIDHYA.COM 。这种工具使用集成使Kimi成为一个功能强大的通用AI助手。再次，Kimi k1.5 在推理和多步骤思考上表现卓越，具备高级的链式思维（chain-of-thought）推理能力 。技术上，Moonshot AI在训练Kimi时大量采用了强化学习策略，让模型学会 规划自己的思路和验证答案正确性 。据其论文，Kimi k1.5在多个基准上达到当前最先 进水平，例如AIME数学77.5分，MATH\\-500达到96.2，Codeforces排名进入前10% 。AR5IV.ORG 。甚至在一些短链推理任务上，Kimi小样本 版本超过了GPT\\-4o和Claude 3.5 Sonnet等强大模型 。AR5IV.ORG 。Kimi k1.5的多模态能力也很突 出：它不仅能看图识字，还能真正“看懂”图像的内容并回答相关问题 。ANALYTCSVIDHYA.COM 。由于免费 开放且无使用限制，Kimik1.5一经推出就在AI社区引发热议，被誉为“媲美OpenAI o1级别的多",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3
                }
            ],
            "layout_bboxes": [],
            "page_idx": 16,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        56,
                        21,
                        560,
                        54
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                21,
                                560,
                                54
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        21,
                                        560,
                                        54
                                    ],
                                    "score": 1.0,
                                    "content": "人、文本生成工具、多语言翻译和垂直领域知识问答等。作为开源领域的领头羊，Llama 3证明了开放模型也能达到顶尖性能，并通过免费提供使用打破了专有模型的垄断。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_16",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        56,
                        69,
                        558,
                        102
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                69,
                                558,
                                102
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        69,
                                        558,
                                        102
                                    ],
                                    "score": 1.0,
                                    "content": "DeepSeek R1：由中国初创公司DeepSeek在2024年底发布的开源大型模型。DeepSeek-R1引 人注目之处在于其卓越的推理能力和开放可获得。据技术报告，R1在许多Benchmark上达到或",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_16",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        42,
                        123,
                        558,
                        439
                    ],
                    "lines": [
                        {
                            "bbox": [
                                42,
                                123,
                                558,
                                439
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        42,
                                        123,
                                        558,
                                        439
                                    ],
                                    "score": 1.0,
                                    "content": "。尤其在数学和编程任务上，DeepSeek-R1表现突出，被评价为“开放源代码领域对闭 源领先模型的有力挑战” 。DeepSeek-V3 底座模型，这是一个Mixture-of-Experts架构的大模型，使基础模型具有超大参 数容量 。在此之上，R1采用了独特的训练流程：首先进行纯强化学习预训练（产生 DeepSeek-R1-Zero模型） ，然后辅以少量有监督SFT稳定语言输出，最后再次通过强 化学习（GRPO算法）强化推理能力 。这种重RL轻监督的训练策略使R1具备 了非凡的链式推理和自主思考能力。技术报告指出，DeepSeek-R1在无需大量人工标注数据的 情况下，单靠自我进化就获得了强大的推理本领 。在长上下文处理方面，R1也经过优 化，可以在较长文本和复杂问题上保持上下文一致。DeepSeek-R1开源了模型权重和代码，实 现了在商用GPU上的部署优化 。社区很快提供了R1的多种蒸馏小模型，甚至通过知识 蒸馏到驯化的Llama和Qwen模型上，使得这些小模型在数学和编码上超越了原本更大的模型 。DeepSeek-R1的主要应用场景包括学术研究（因为其推理能力强，适合科学问 答）、高级编程助手（在代码挑战上表现优异）、以及需要严谨逐步推理的业务场景（如法律分 析助手等）。作为一个开源项目，R1还受到社区安全测试的挑战并不断完善，是中国在大模型 核心技术上取得突破的代表之一。",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_16",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        42,
                        454,
                        558,
                        781
                    ],
                    "lines": [
                        {
                            "bbox": [
                                42,
                                454,
                                558,
                                781
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        42,
                                        454,
                                        558,
                                        781
                                    ],
                                    "score": 1.0,
                                    "content": "$\\bullet$ Kimi k1.5：Moonshot AI公司（成立于2023年的中国AI新创）推出的最新LLM版本 。Kimi k1.5是一款多模态开源模型，免费向公众提供使用，并提供网页版聊天 接口 。ANALYTCSVIDHYA.COM 。ANALYTCSVIDHYA.COM 。它有几个显著特点：首先是超长上下文——Kimi k1.5支 持128K tokens的上下文窗口 。ANALYTCSVIDHYA.COM ，能一次处理海量文本或多个文件输入。其次， Kimi k1.5内置了实时网页搜索和多文件分析能力，可在聊天中直接检索信息或解析PDF/图片等 文件 。例如，用户可以让它同时分析多达50个文档（PDF、Word、图片等）并 综合给出结果 。ANALYTCSVIDHYA.COM 。这种工具使用集成使Kimi成为一个功能强大的通用AI助手。再次，Kimi k1.5 在推理和多步骤思考上表现卓越，具备高级的链式思维（chain-of-thought）推理能力 。技术上，Moonshot AI在训练Kimi时大量采用了强化学习策略，让模型学会 规划自己的思路和验证答案正确性 。据其论文，Kimi k1.5在多个基准上达到当前最先 进水平，例如AIME数学77.5分，MATH\\-500达到96.2，Codeforces排名进入前10% 。AR5IV.ORG 。甚至在一些短链推理任务上，Kimi小样本 版本超过了GPT\\-4o和Claude 3.5 Sonnet等强大模型 。AR5IV.ORG 。Kimi k1.5的多模态能力也很突 出：它不仅能看图识字，还能真正“看懂”图像的内容并回答相关问题 。ANALYTCSVIDHYA.COM 。由于免费 开放且无使用限制，Kimik1.5一经推出就在AI社区引发热议，被誉为“媲美OpenAI o1级别的多",
                                    "type": "text"
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "index": 3,
                    "page_num": "page_16",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        },
        {
            "preproc_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        56,
                        20,
                        553,
                        96
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                20,
                                553,
                                96
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        20,
                                        553,
                                        96
                                    ],
                                    "score": 1.0,
                                    "content": "模态开源模型”  。主要应用场景包括通用问答助手、教育学习（可帮助解题并给 出过程）、企业文档分析（一次性分析大量内部文档）以及需要图文结合的复杂信息处理等。 Kimi k1.5展示了开源模型通过创新训练（强化学习）与功能集成（检索+多模态）也能达到顶 尖水平，为开源阵营在2025年的竞争增添了一名强力选手。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        125,
                        558,
                        327
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                125,
                                558,
                                327
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        125,
                                        558,
                                        327
                                    ],
                                    "score": 1.0,
                                    "content": "综上，2024年以来的大模型百花齐放：OpenAI的GPT\\-4o、O1、O3系列在闭源领域不断突破推理 极限；Google的Gemini 2.0强调多模态融合与内部思考；Anthropic的Claude 3.5提升速度并打开 超长上下文和视觉能力的新局面；Meta的Llama 3坚持开源，参数冲上百亿量级并真正走入大众产 品；DeepSeek R1和Kimi k1.5则代表了中国团队的创新力量，通过强化学习等新思路在推理和多模 态上后来居上。这些模型各有侧重：有的追求更强的推理逻辑，有的扩展模态边界，有的优化长文 本处理，有的融合工具使用。但共同点是参数更庞大、训练数据更丰富、应用场景更贴近实际需 求。不论开源或闭源，它们都推动着AI语言模型往“更智能、更有用、更可靠”的方向发展。展望未 来，随着研究的深入和算力的提升，我们有理由期待出现具备更强推理能力、更长时序记忆、甚至 一定自主学习能力的新一代大模型，进一步缩小与通用人工智能（AGI）的差距，为各行各业带来革 命性的智能助理。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "text",
                    "bbox": [
                        41,
                        347,
                        64,
                        360
                    ],
                    "lines": [
                        {
                            "bbox": [
                                41,
                                347,
                                64,
                                360
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        41,
                                        347,
                                        64,
                                        360
                                    ],
                                    "score": 1.0,
                                    "content": "(完)",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2
                }
            ],
            "layout_bboxes": [],
            "page_idx": 17,
            "page_size": [
                595.9199829101562,
                842.8800048828125
            ],
            "_layout_tree": [],
            "images": [],
            "tables": [],
            "interline_equations": [],
            "discarded_blocks": [
                {
                    "type": "discarded",
                    "bbox": [
                        533,
                        826,
                        558,
                        835
                    ],
                    "lines": [
                        {
                            "bbox": [
                                533,
                                826,
                                558,
                                835
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        533,
                                        826,
                                        558,
                                        835
                                    ],
                                    "score": 1.0,
                                    "content": "",
                                    "type": "text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "need_drop": false,
            "drop_reason": [],
            "para_blocks": [
                {
                    "type": "text",
                    "bbox": [
                        56,
                        20,
                        553,
                        96
                    ],
                    "lines": [
                        {
                            "bbox": [
                                56,
                                20,
                                553,
                                96
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        56,
                                        20,
                                        553,
                                        96
                                    ],
                                    "score": 1.0,
                                    "content": "模态开源模型”  。主要应用场景包括通用问答助手、教育学习（可帮助解题并给 出过程）、企业文档分析（一次性分析大量内部文档）以及需要图文结合的复杂信息处理等。 Kimi k1.5展示了开源模型通过创新训练（强化学习）与功能集成（检索+多模态）也能达到顶 尖水平，为开源阵营在2025年的竞争增添了一名强力选手。",
                                    "type": "text"
                                }
                            ],
                            "index": 0
                        }
                    ],
                    "index": 0,
                    "page_num": "page_17",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        32,
                        125,
                        558,
                        327
                    ],
                    "lines": [
                        {
                            "bbox": [
                                32,
                                125,
                                558,
                                327
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        32,
                                        125,
                                        558,
                                        327
                                    ],
                                    "score": 1.0,
                                    "content": "综上，2024年以来的大模型百花齐放：OpenAI的GPT\\-4o、O1、O3系列在闭源领域不断突破推理 极限；Google的Gemini 2.0强调多模态融合与内部思考；Anthropic的Claude 3.5提升速度并打开 超长上下文和视觉能力的新局面；Meta的Llama 3坚持开源，参数冲上百亿量级并真正走入大众产 品；DeepSeek R1和Kimi k1.5则代表了中国团队的创新力量，通过强化学习等新思路在推理和多模 态上后来居上。这些模型各有侧重：有的追求更强的推理逻辑，有的扩展模态边界，有的优化长文 本处理，有的融合工具使用。但共同点是参数更庞大、训练数据更丰富、应用场景更贴近实际需 求。不论开源或闭源，它们都推动着AI语言模型往“更智能、更有用、更可靠”的方向发展。展望未 来，随着研究的深入和算力的提升，我们有理由期待出现具备更强推理能力、更长时序记忆、甚至 一定自主学习能力的新一代大模型，进一步缩小与通用人工智能（AGI）的差距，为各行各业带来革 命性的智能助理。",
                                    "type": "text"
                                }
                            ],
                            "index": 1
                        }
                    ],
                    "index": 1,
                    "page_num": "page_17",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                },
                {
                    "type": "text",
                    "bbox": [
                        41,
                        347,
                        64,
                        360
                    ],
                    "lines": [
                        {
                            "bbox": [
                                41,
                                347,
                                64,
                                360
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        41,
                                        347,
                                        64,
                                        360
                                    ],
                                    "score": 1.0,
                                    "content": "(完)",
                                    "type": "text"
                                }
                            ],
                            "index": 2
                        }
                    ],
                    "index": 2,
                    "page_num": "page_17",
                    "page_size": [
                        595.9199829101562,
                        842.8800048828125
                    ]
                }
            ]
        }
    ],
    "_parse_type": "ocr",
    "_version_name": "1.1.0"
}